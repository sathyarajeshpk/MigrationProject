{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b419d7-88a4-4408-a1fb-cec1504fc31b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ **Daily Data Pipeline - Execution Guide**\n",
    "## Bronze to Silver Layer Processing with Data Quality\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **PRODUCTION WORKFLOW - Execute in This Order**\n",
    "\n",
    "### **Phase 1: Configuration** ‚è±Ô∏è ~5 seconds\n",
    "\n",
    "**Cell 1 - Configuration (Auto-discover databases and tables)**\n",
    "* Scans `bronze/mysql/` to discover all databases automatically\n",
    "* Auto-discovers all tables within each database\n",
    "* Creates `all_database_configs` variable with paths\n",
    "* **Output:** 2 databases (retail_db, students_db), 8 tables total\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 2: Data Quality Analysis** ‚è±Ô∏è ~30 seconds\n",
    "\n",
    "**Cell 2 - Analyze MySQL Bronze Data Quality**\n",
    "* Checks all MySQL tables for nulls, duplicates, data types\n",
    "* Displays sample records and schema\n",
    "* Generates quality report\n",
    "* **Issues Found:** 34 duplicates, null values in several tables\n",
    "\n",
    "**Cell 3 - Analyze Event Hub Bronze Data Quality**\n",
    "* Analyzes Event Hub AVRO data structure\n",
    "* Checks Body field (JSON content), SequenceNumber, Offset\n",
    "* Shows load date distribution\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 3: Data Cleaning** ‚è±Ô∏è ~1 minute\n",
    "\n",
    "**Cell 4 - Define Cleaning Rules**\n",
    "* Global rules: remove duplicates, trim strings, handle nulls\n",
    "* Table-specific rules: email validation, required fields, dedupe keys\n",
    "\n",
    "**Cell 5 - Apply Cleaning to MySQL Data**\n",
    "* Removes 34 duplicates (27% reduction)\n",
    "* Trims strings, filters invalid emails, removes null required fields\n",
    "* Creates temp views: `cleaned_{db_name}_{table_name}`\n",
    "* **Results:** 124 ‚Üí 90 records\n",
    "\n",
    "**Cell 6 - Apply Cleaning to Event Hub Data**\n",
    "* Removes duplicates, parses JSON Body field\n",
    "* Decodes base64 content, flattens nested JSON\n",
    "* Creates temp view: `cleaned_eventhub_events`\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 4: Validation** ‚è±Ô∏è ~20 seconds\n",
    "\n",
    "**Cell 7 - Validate Cleaned Data Quality**\n",
    "* Verifies no nulls in required fields\n",
    "* Confirms no duplicates remain\n",
    "* **Result:** ‚úÖ 8/8 tables passed validation\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 5: Event Hub Processing** ‚è±Ô∏è ~1 minute\n",
    "\n",
    "**Cell 8 - Event Hub ‚Üí Bronze**\n",
    "* Reads AVRO files from `streamingingestionsathya/eventhub/`\n",
    "* Uses Auto Loader with `recursiveFileLookup`\n",
    "* Processes nested folders: `partition/year/month/day/hour/minute`\n",
    "* Writes to `bronze/eventhub/eventhub_events/` as Delta\n",
    "* Partitions by load_date, uses checkpoint\n",
    "\n",
    "**Cell 9 - Event Hub Bronze ‚Üí Silver**\n",
    "* Reads from bronze Event Hub Delta table\n",
    "* Adds processing metadata\n",
    "* Writes to `silver/eventhub/eventhub_events/`\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 6: MySQL to Silver** ‚è±Ô∏è ~30 seconds\n",
    "\n",
    "**Cell 10 - MySQL Cleaned Data ‚Üí Silver**\n",
    "* Reads CLEANED data from temp views (Cell 5)\n",
    "* Adds processing metadata\n",
    "* Writes to `silver/mysql/{database}/{table}/` as Delta\n",
    "* **Output:** 90 clean, validated records\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 7: Verification** ‚è±Ô∏è ~10 seconds\n",
    "\n",
    "**Cell 11 - Complete Silver Layer Summary**\n",
    "* Verifies all MySQL databases and Event Hub in silver\n",
    "* Shows record counts per table\n",
    "* **Final:** 3 sources, 9 tables, 92 total records\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è **TOTAL DURATION: ~4 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ **SCHEDULING RECOMMENDATIONS**\n",
    "\n",
    "### **Option 1: Daily Batch Processing** ‚≠ê RECOMMENDED\n",
    "\n",
    "**Schedule:** Daily at 2:00 AM  \n",
    "**Cron:** `0 2 * * *`  \n",
    "**Run Cells:** 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5 ‚Üí 6 ‚Üí 7 ‚Üí 8 ‚Üí 9 ‚Üí 10 ‚Üí 11\n",
    "\n",
    "**Best For:**\n",
    "* Standard daily data processing\n",
    "* Both MySQL and Event Hub updated once per day\n",
    "* Cost-effective (runs once per day)\n",
    "\n",
    "**Databricks Workflow Setup:**\n",
    "```\n",
    "Task 1: Configuration & Cleaning\n",
    "  Cells: 1, 2, 3, 4, 5, 6, 7\n",
    "  Duration: ~2 min\n",
    "  Dependencies: None\n",
    "\n",
    "Task 2: Event Hub Processing\n",
    "  Cells: 8, 9\n",
    "  Duration: ~1 min\n",
    "  Dependencies: Task 1\n",
    "\n",
    "Task 3: MySQL to Silver\n",
    "  Cell: 10\n",
    "  Duration: ~30 sec\n",
    "  Dependencies: Task 1\n",
    "\n",
    "Task 4: Verification\n",
    "  Cell: 11\n",
    "  Duration: ~10 sec\n",
    "  Dependencies: Task 2, Task 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: Frequent Event Hub, Daily MySQL**\n",
    "\n",
    "**Event Hub Schedule:** Every hour  \n",
    "**Cron:** `0 * * * *`  \n",
    "**Run Cells:** 1 ‚Üí 8 ‚Üí 9 ‚Üí 11\n",
    "\n",
    "**MySQL Schedule:** Daily at 2:00 AM  \n",
    "**Cron:** `0 2 * * *`  \n",
    "**Run Cells:** 1 ‚Üí 2 ‚Üí 4 ‚Üí 5 ‚Üí 7 ‚Üí 10 ‚Üí 11\n",
    "\n",
    "**Best For:**\n",
    "* Near real-time Event Hub processing\n",
    "* Batch MySQL processing\n",
    "* Different SLAs for different sources\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 3: Skip Analysis (After Initial Setup)**\n",
    "\n",
    "**Schedule:** Daily at 2:00 AM  \n",
    "**Run Cells:** 1 ‚Üí 4 ‚Üí 5 ‚Üí 6 ‚Üí 7 ‚Üí 8 ‚Üí 9 ‚Üí 10 ‚Üí 11\n",
    "\n",
    "**Best For:**\n",
    "* After initial data quality assessment\n",
    "* Faster execution (skip analysis Cells 2-3)\n",
    "* Stable data sources with known quality\n",
    "\n",
    "**Note:** Run Cells 2-3 weekly/monthly to monitor quality trends\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **DATA QUALITY RESULTS**\n",
    "\n",
    "### **Bronze Layer (Raw):**\n",
    "* Total records: 124\n",
    "* Issues: 34 duplicates, null values in multiple tables\n",
    "\n",
    "### **Silver Layer (Cleaned):**\n",
    "* Total records: 92 (90 MySQL + 2 Event Hub)\n",
    "* Quality: ‚úÖ All validations passed (8/8 tables)\n",
    "* Improvement: 27% reduction in bad data\n",
    "\n",
    "### **Specific Improvements:**\n",
    "* customer_details: 9 ‚Üí 6 records (3 duplicates removed)\n",
    "* orders: 42 ‚Üí 21 records (21 duplicates removed)\n",
    "* users: 20 ‚Üí 10 records (10 duplicates removed)\n",
    "\n",
    "---\n",
    "\n",
    "## üìç **STORAGE LOCATIONS**\n",
    "\n",
    "**Bronze Layer:**\n",
    "* MySQL: `bronze/mysql/{database}/{table}/load_date=YYYY-MM-DD/`\n",
    "* Event Hub: `bronze/eventhub/eventhub_events/load_date=YYYY-MM-DD/`\n",
    "\n",
    "**Silver Layer:**\n",
    "* MySQL: `silver/mysql/{database}/{table}/`\n",
    "* Event Hub: `silver/eventhub/eventhub_events/`\n",
    "\n",
    "**Checkpoints:**\n",
    "* Event Hub to Bronze: `checkpoints/eventhub_to_bronze/eventhub_events/`\n",
    "* Event Hub to Silver: `checkpoints/bronze_to_silver/eventhub/eventhub_events/`\n",
    "\n",
    "---\n",
    "\n",
    "## üîë **KEY FEATURES**\n",
    "\n",
    "‚úÖ **Fully Dynamic** - Auto-discovers new databases and tables  \n",
    "‚úÖ **Incremental** - Only processes new files (checkpoint-based)  \n",
    "‚úÖ **Data Quality** - Analysis, cleaning, validation built-in  \n",
    "‚úÖ **Multi-Source** - Handles MySQL + Event Hub seamlessly  \n",
    "‚úÖ **Scalable** - Add new sources without code changes  \n",
    "‚úÖ **Monitored** - Clear success/failure indicators  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° **PERFORMANCE NOTES**\n",
    "\n",
    "**First Run:**\n",
    "* Duration: ~4 minutes\n",
    "* Processes ALL existing data\n",
    "* Creates checkpoints\n",
    "\n",
    "**Subsequent Runs:**\n",
    "* Duration: ~30 seconds (if no new data)\n",
    "* Only processes NEW files\n",
    "* Fast checkpoint lookup\n",
    "\n",
    "**Auto Loader Behavior:**\n",
    "* Tracks files by path/name only (not content)\n",
    "* Modified files with same name are SKIPPED\n",
    "* Recommendation: Use append-only pattern with unique filenames\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **NOTEBOOK PATH**\n",
    "\n",
    "`/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup`\n",
    "\n",
    "---\n",
    "\n",
    "## üìß **CONTACT**\n",
    "\n",
    "**Owner:** sathyarajeshpk@gmail.com  \n",
    "**Workspace:** datamigrationsathya (Azure)  \n",
    "**Compute:** Serverless Interactive Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00e704b-a60a-4802-b00a-a675a46c1815",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration - Define paths and settings"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULLY DYNAMIC CONFIGURATION\n",
    "# Auto-discovers databases AND tables\n",
    "# ============================================\n",
    "\n",
    "# Base storage account and container\n",
    "storage_account = \"datamigrationsathya\"\n",
    "container = \"datalake\"\n",
    "\n",
    "# Path structure configuration\n",
    "layer = \"bronze\"  # bronze, silver, gold\n",
    "source_system = \"mysql\"  # mysql, postgres, etc.\n",
    "\n",
    "# Base path for the source system\n",
    "source_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/{source_system}/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTO-DISCOVERING DATABASES AND TABLES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSource system: {source_system}\")\n",
    "print(f\"Base path: {source_base_path}\")\n",
    "\n",
    "# ============================================\n",
    "# AUTO-DISCOVER ALL DATABASES\n",
    "# ============================================\n",
    "\n",
    "all_database_configs = []\n",
    "\n",
    "try:\n",
    "    # List all database folders under bronze/mysql/\n",
    "    database_folders = dbutils.fs.ls(source_base_path)\n",
    "    \n",
    "    print(f\"\\nFound {len(database_folders)} database(s):\\n\")\n",
    "    \n",
    "    for db_folder in database_folders:\n",
    "        if db_folder.isDir():\n",
    "            database_name = db_folder.name.rstrip('/')\n",
    "            \n",
    "            # Construct paths for this database\n",
    "            bronze_path = f\"{source_base_path}{database_name}/\"\n",
    "            silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "            checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "            \n",
    "            # Auto-discover tables in this database\n",
    "            try:\n",
    "                table_folders = dbutils.fs.ls(bronze_path)\n",
    "                tables = [t.name.rstrip('/') for t in table_folders if t.isDir()]\n",
    "                \n",
    "                if tables:  # Only add if tables exist\n",
    "                    all_database_configs.append({\n",
    "                        \"database_name\": database_name,\n",
    "                        \"bronze_path\": bronze_path,\n",
    "                        \"silver_path\": silver_path,\n",
    "                        \"checkpoint_path\": checkpoint_path,\n",
    "                        \"tables\": tables\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  ‚úì {database_name}: {len(tables)} table(s)\")\n",
    "                    for table in tables:\n",
    "                        print(f\"      - {table}\")\n",
    "                    print()\n",
    "                else:\n",
    "                    print(f\"  ‚ö† {database_name}: No tables found (skipping)\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {database_name}: Error reading tables - {str(e)}\\n\")\n",
    "                continue\n",
    "    \n",
    "    if not all_database_configs:\n",
    "        print(\"\\n‚ö†Ô∏è  No databases with tables found!\")\n",
    "        print(\"Please check your bronze layer structure.\")\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"SUMMARY: {len(all_database_configs)} database(s) ready to process\")\n",
    "        total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "        print(f\"Total tables across all databases: {total_tables}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error discovering databases: {str(e)}\")\n",
    "    print(\"\\nFalling back to manual configuration...\")\n",
    "    \n",
    "    # Fallback: Manual configuration\n",
    "    database_name = \"retail_db\"\n",
    "    bronze_base_path = f\"{source_base_path}{database_name}/\"\n",
    "    silver_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "    checkpoint_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "    \n",
    "    # Auto-discover tables\n",
    "    try:\n",
    "        folders = dbutils.fs.ls(bronze_base_path)\n",
    "        tables_to_process = [folder.name.rstrip('/') for folder in folders if folder.isDir()]\n",
    "        print(f\"Using manual database: {database_name}\")\n",
    "        print(f\"Found {len(tables_to_process)} tables: {tables_to_process}\")\n",
    "    except:\n",
    "        tables_to_process = [\"customer_details\"]\n",
    "        print(f\"Using fallback table list: {tables_to_process}\")\n",
    "    \n",
    "    # Create single database config for backward compatibility\n",
    "    all_database_configs = [{\n",
    "        \"database_name\": database_name,\n",
    "        \"bronze_path\": bronze_base_path,\n",
    "        \"silver_path\": silver_base_path,\n",
    "        \"checkpoint_path\": checkpoint_base_path,\n",
    "        \"tables\": tables_to_process\n",
    "    }]\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete!\")\n",
    "print(\"\\nNote: Auto Loader will recursively process all files in subdirectories (e.g., load_date partitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57c5cb0-3bd8-430e-8bb2-efe87fbea5eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ANALYSIS: Analyze MySQL Bronze Data Quality"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive data quality analysis for MySQL bronze layer\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, sum as spark_sum, avg, min as spark_min, max as spark_max\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MYSQL BRONZE LAYER - DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 6 (Configuration) first!\")\n",
    "else:\n",
    "    quality_report = []\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for table_name in tables:\n",
    "            print(f\"\\nüìä Analyzing: {table_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            try:\n",
    "                # Read bronze data\n",
    "                df = spark.read.parquet(f\"{bronze_path}{table_name}/\")\n",
    "                \n",
    "                total_records = df.count()\n",
    "                total_columns = len(df.columns)\n",
    "                \n",
    "                print(f\"  Total Records: {total_records:,}\")\n",
    "                print(f\"  Total Columns: {total_columns}\")\n",
    "                \n",
    "                # 1. NULL ANALYSIS\n",
    "                print(f\"\\n  üìã Null Analysis:\")\n",
    "                null_counts = df.select([spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]).collect()[0]\n",
    "                \n",
    "                has_nulls = False\n",
    "                for col_name in df.columns:\n",
    "                    null_count = null_counts[col_name]\n",
    "                    if null_count > 0:\n",
    "                        null_pct = (null_count / total_records) * 100\n",
    "                        print(f\"    ‚ö† {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "                        has_nulls = True\n",
    "                \n",
    "                if not has_nulls:\n",
    "                    print(f\"    ‚úì No null values found\")\n",
    "                \n",
    "                # 2. DUPLICATE ANALYSIS\n",
    "                print(f\"\\n  üîÑ Duplicate Analysis:\")\n",
    "                duplicate_count = total_records - df.dropDuplicates().count()\n",
    "                if duplicate_count > 0:\n",
    "                    print(f\"    ‚ö† Found {duplicate_count:,} duplicate records ({(duplicate_count/total_records)*100:.1f}%)\")\n",
    "                else:\n",
    "                    print(f\"    ‚úì No duplicates found\")\n",
    "                \n",
    "                # 3. DATA TYPE ANALYSIS\n",
    "                print(f\"\\n  üìù Schema:\")\n",
    "                for field in df.schema.fields:\n",
    "                    print(f\"    - {field.name}: {field.dataType}\")\n",
    "                \n",
    "                # 4. SAMPLE DATA\n",
    "                print(f\"\\n  üìÑ Sample Records (first 3):\")\n",
    "                display(df.limit(3))\n",
    "                \n",
    "                # Store quality metrics\n",
    "                quality_report.append({\n",
    "                    'database': db_name,\n",
    "                    'table': table_name,\n",
    "                    'total_records': total_records,\n",
    "                    'total_columns': total_columns,\n",
    "                    'has_nulls': has_nulls,\n",
    "                    'duplicate_count': duplicate_count\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error analyzing {table_name}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Summary Report\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"QUALITY ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for report in quality_report:\n",
    "        status = \"‚ö† Issues Found\" if (report['has_nulls'] or report['duplicate_count'] > 0) else \"‚úì Clean\"\n",
    "        print(f\"\\n{report['database']}.{report['table']}: {status}\")\n",
    "        print(f\"  Records: {report['total_records']:,}\")\n",
    "        if report['has_nulls']:\n",
    "            print(f\"  ‚ö† Has null values\")\n",
    "        if report['duplicate_count'] > 0:\n",
    "            print(f\"  ‚ö† Has {report['duplicate_count']:,} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e10db409-0622-46c9-ba23-6d654c55e832",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ANALYSIS: Analyze Event Hub Bronze Data Quality"
    }
   },
   "outputs": [],
   "source": [
    "# Data quality analysis for Event Hub bronze layer\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, length, size\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB BRONZE LAYER - DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "eventhub_name = \"eventhub_events\"\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "\n",
    "try:\n",
    "    df_eh = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    \n",
    "    total_records = df_eh.count()\n",
    "    print(f\"\\nTotal Records: {total_records:,}\")\n",
    "    print(f\"Total Columns: {len(df_eh.columns)}\")\n",
    "    \n",
    "    # 1. NULL ANALYSIS\n",
    "    print(f\"\\nüìã Null Analysis:\")\n",
    "    null_counts = df_eh.select([spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_eh.columns]).collect()[0]\n",
    "    \n",
    "    for col_name in df_eh.columns:\n",
    "        null_count = null_counts[col_name]\n",
    "        if null_count > 0:\n",
    "            null_pct = (null_count / total_records) * 100\n",
    "            print(f\"  ‚ö† {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "    \n",
    "    # 2. BODY FIELD ANALYSIS (contains JSON data)\n",
    "    print(f\"\\nüì¶ Body Field Analysis:\")\n",
    "    if 'Body' in df_eh.columns:\n",
    "        # Check for empty bodies\n",
    "        empty_bodies = df_eh.filter(col('Body').isNull()).count()\n",
    "        print(f\"  Empty bodies: {empty_bodies}\")\n",
    "        \n",
    "        # Sample body content\n",
    "        print(f\"\\n  Sample Body content (first record):\")\n",
    "        sample_body = df_eh.select('Body').limit(1).collect()[0]['Body']\n",
    "        if sample_body:\n",
    "            import base64\n",
    "            decoded = base64.b64decode(sample_body).decode('utf-8')\n",
    "            print(f\"  {decoded[:200]}...\")\n",
    "    \n",
    "    # 3. DUPLICATE ANALYSIS\n",
    "    print(f\"\\nüîÑ Duplicate Analysis:\")\n",
    "    duplicate_count = total_records - df_eh.dropDuplicates(['SequenceNumber', 'Offset']).count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"  ‚ö† Found {duplicate_count:,} duplicate records\")\n",
    "    else:\n",
    "        print(f\"  ‚úì No duplicates found\")\n",
    "    \n",
    "    # 4. LOAD DATE DISTRIBUTION\n",
    "    print(f\"\\nüìÖ Load Date Distribution:\")\n",
    "    if 'load_date' in df_eh.columns:\n",
    "        df_eh.groupBy('load_date').count().orderBy('load_date').show()\n",
    "    \n",
    "    # 5. SCHEMA\n",
    "    print(f\"\\nüìù Schema:\")\n",
    "    df_eh.printSchema()\n",
    "    \n",
    "    # 6. SAMPLE DATA\n",
    "    print(f\"\\nüìÑ Sample Records:\")\n",
    "    display(df_eh.limit(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    print(\"Make sure Event Hub data has been copied to bronze layer (run Cell 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa58942b-59f7-4a46-b590-d73c81d90286",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLEANING: Define cleaning rules and transformations"
    }
   },
   "outputs": [],
   "source": [
    "# Define data cleaning rules and transformations\n",
    "\n",
    "from pyspark.sql.functions import col, trim, upper, lower, regexp_replace, to_timestamp, coalesce, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA CLEANING RULES DEFINITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================\n",
    "# CLEANING RULES CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "cleaning_rules = {\n",
    "    # Global rules applied to all tables\n",
    "    'global': {\n",
    "        'remove_duplicates': True,\n",
    "        'trim_strings': True,\n",
    "        'handle_nulls': True,\n",
    "        'standardize_dates': True\n",
    "    },\n",
    "    \n",
    "    # Table-specific rules\n",
    "    'table_specific': {\n",
    "        'customer_details': {\n",
    "            'required_fields': ['customer_id', 'customer_name'],\n",
    "            'email_validation': True,\n",
    "            'dedupe_key': ['customer_id']\n",
    "        },\n",
    "        'orders': {\n",
    "            'required_fields': ['order_id'],\n",
    "            'dedupe_key': ['order_id'],\n",
    "            'filter_negative_amounts': True\n",
    "        },\n",
    "        'users': {\n",
    "            'required_fields': ['user_id', 'name'],\n",
    "            'dedupe_key': ['user_id']\n",
    "        },\n",
    "        'eventhub_events': {\n",
    "            'required_fields': ['SequenceNumber', 'Offset'],\n",
    "            'dedupe_key': ['SequenceNumber', 'Offset'],\n",
    "            'parse_body_json': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Cleaning rules defined:\")\n",
    "print(f\"\\nüìã Global Rules:\")\n",
    "for rule, enabled in cleaning_rules['global'].items():\n",
    "    print(f\"  - {rule}: {enabled}\")\n",
    "\n",
    "print(f\"\\nüìã Table-Specific Rules:\")\n",
    "for table, rules in cleaning_rules['table_specific'].items():\n",
    "    print(f\"\\n  {table}:\")\n",
    "    for rule, value in rules.items():\n",
    "        print(f\"    - {rule}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Rules ready to apply!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4b2166-8814-4cd3-aaa8-abeaca326196",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLEANING: Apply transformations to MySQL data"
    }
   },
   "outputs": [],
   "source": [
    "# Apply cleaning transformations to MySQL bronze data\n",
    "\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, when, length, coalesce\n",
    "\n",
    "def clean_mysql_table(df, table_name, rules):\n",
    "    \"\"\"\n",
    "    Apply cleaning transformations to a DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df\n",
    "    \n",
    "    # Get table-specific rules\n",
    "    table_rules = rules['table_specific'].get(table_name, {})\n",
    "    \n",
    "    # 1. REMOVE DUPLICATES\n",
    "    if rules['global']['remove_duplicates']:\n",
    "        dedupe_key = table_rules.get('dedupe_key', None)\n",
    "        if dedupe_key:\n",
    "            initial_count = df_clean.count()\n",
    "            df_clean = df_clean.dropDuplicates(dedupe_key)\n",
    "            removed = initial_count - df_clean.count()\n",
    "            if removed > 0:\n",
    "                print(f\"    ‚úì Removed {removed:,} duplicates based on {dedupe_key}\")\n",
    "    \n",
    "    # 2. TRIM STRING COLUMNS\n",
    "    if rules['global']['trim_strings']:\n",
    "        string_cols = [field.name for field in df_clean.schema.fields if str(field.dataType) == 'StringType']\n",
    "        for col_name in string_cols:\n",
    "            df_clean = df_clean.withColumn(col_name, trim(col(col_name)))\n",
    "        if string_cols:\n",
    "            print(f\"    ‚úì Trimmed {len(string_cols)} string columns\")\n",
    "    \n",
    "    # 3. FILTER REQUIRED FIELDS\n",
    "    required_fields = table_rules.get('required_fields', [])\n",
    "    if required_fields:\n",
    "        initial_count = df_clean.count()\n",
    "        for field in required_fields:\n",
    "            if field in df_clean.columns:\n",
    "                df_clean = df_clean.filter(col(field).isNotNull())\n",
    "        removed = initial_count - df_clean.count()\n",
    "        if removed > 0:\n",
    "            print(f\"    ‚úì Filtered {removed:,} records with null required fields\")\n",
    "    \n",
    "    # 4. EMAIL VALIDATION (if applicable)\n",
    "    if table_rules.get('email_validation', False):\n",
    "        email_cols = [c for c in df_clean.columns if 'email' in c.lower()]\n",
    "        for email_col in email_cols:\n",
    "            initial_count = df_clean.count()\n",
    "            df_clean = df_clean.filter(\n",
    "                (col(email_col).isNull()) | \n",
    "                (col(email_col).rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'))\n",
    "            )\n",
    "            removed = initial_count - df_clean.count()\n",
    "            if removed > 0:\n",
    "                print(f\"    ‚úì Filtered {removed:,} records with invalid emails\")\n",
    "    \n",
    "    # 5. FILTER NEGATIVE AMOUNTS (if applicable)\n",
    "    if table_rules.get('filter_negative_amounts', False):\n",
    "        amount_cols = [c for c in df_clean.columns if 'amount' in c.lower() or 'price' in c.lower()]\n",
    "        for amount_col in amount_cols:\n",
    "            if amount_col in df_clean.columns:\n",
    "                initial_count = df_clean.count()\n",
    "                df_clean = df_clean.filter((col(amount_col).isNull()) | (col(amount_col) >= 0))\n",
    "                removed = initial_count - df_clean.count()\n",
    "                if removed > 0:\n",
    "                    print(f\"    ‚úì Filtered {removed:,} records with negative {amount_col}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLYING CLEANING TRANSFORMATIONS - MYSQL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 6 (Configuration) first!\")\n",
    "else:\n",
    "    cleaning_summary = []\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üßπ Cleaning: {table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read bronze data\n",
    "                df_bronze = spark.read.parquet(f\"{bronze_path}{table_name}/\")\n",
    "                initial_count = df_bronze.count()\n",
    "                \n",
    "                # Apply cleaning\n",
    "                df_clean = clean_mysql_table(df_bronze, table_name, cleaning_rules)\n",
    "                final_count = df_clean.count()\n",
    "                \n",
    "                removed = initial_count - final_count\n",
    "                retention_pct = (final_count / initial_count) * 100 if initial_count > 0 else 0\n",
    "                \n",
    "                print(f\"    üìä Initial: {initial_count:,} ‚Üí Final: {final_count:,} ({retention_pct:.1f}% retained)\")\n",
    "                \n",
    "                # Store cleaned DataFrame for later use\n",
    "                df_clean.createOrReplaceTempView(f\"cleaned_{db_name}_{table_name}\")\n",
    "                \n",
    "                cleaning_summary.append({\n",
    "                    'database': db_name,\n",
    "                    'table': table_name,\n",
    "                    'initial': initial_count,\n",
    "                    'final': final_count,\n",
    "                    'removed': removed\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {str(e)[:100]}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"CLEANING SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    total_initial = sum(s['initial'] for s in cleaning_summary)\n",
    "    total_final = sum(s['final'] for s in cleaning_summary)\n",
    "    total_removed = sum(s['removed'] for s in cleaning_summary)\n",
    "    \n",
    "    for summary in cleaning_summary:\n",
    "        print(f\"\\n{summary['database']}.{summary['table']}:\")\n",
    "        print(f\"  Initial: {summary['initial']:,}\")\n",
    "        print(f\"  Final: {summary['final']:,}\")\n",
    "        print(f\"  Removed: {summary['removed']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TOTAL: {total_initial:,} ‚Üí {total_final:,} (removed {total_removed:,})\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bf71bc-35f8-4293-bcc1-94621a81231e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLEANING: Apply transformations to Event Hub data"
    }
   },
   "outputs": [],
   "source": [
    "# Apply cleaning transformations to Event Hub bronze data\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, schema_of_json, base64, unbase64\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLYING CLEANING TRANSFORMATIONS - EVENT HUB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "eventhub_name = \"eventhub_events\"\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "\n",
    "try:\n",
    "    df_eh = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    initial_count = df_eh.count()\n",
    "    \n",
    "    print(f\"\\nInitial records: {initial_count:,}\")\n",
    "    \n",
    "    # 1. REMOVE DUPLICATES\n",
    "    print(f\"\\nüßπ Cleaning steps:\")\n",
    "    df_clean = df_eh.dropDuplicates(['SequenceNumber', 'Offset'])\n",
    "    removed_dupes = initial_count - df_clean.count()\n",
    "    if removed_dupes > 0:\n",
    "        print(f\"  ‚úì Removed {removed_dupes:,} duplicates\")\n",
    "    else:\n",
    "        print(f\"  ‚úì No duplicates found\")\n",
    "    \n",
    "    # 2. FILTER NULL REQUIRED FIELDS\n",
    "    df_clean = df_clean.filter(\n",
    "        col('SequenceNumber').isNotNull() & \n",
    "        col('Offset').isNotNull()\n",
    "    )\n",
    "    removed_nulls = df_eh.count() - df_clean.count() - removed_dupes\n",
    "    if removed_nulls > 0:\n",
    "        print(f\"  ‚úì Filtered {removed_nulls:,} records with null required fields\")\n",
    "    \n",
    "    # 3. PARSE BODY JSON (if Body field exists)\n",
    "    if 'Body' in df_clean.columns:\n",
    "        print(f\"\\n  üì¶ Parsing Body JSON field...\")\n",
    "        \n",
    "        # Decode base64 Body to string\n",
    "        from pyspark.sql.functions import expr\n",
    "        df_clean = df_clean.withColumn('body_string', expr(\"cast(unbase64(Body) as string)\"))\n",
    "        \n",
    "        # Try to infer JSON schema from sample\n",
    "        sample_body = df_clean.select('body_string').filter(col('body_string').isNotNull()).limit(1).collect()\n",
    "        \n",
    "        if sample_body and sample_body[0]['body_string']:\n",
    "            try:\n",
    "                # Infer schema from sample JSON\n",
    "                json_schema = schema_of_json(sample_body[0]['body_string'])\n",
    "                \n",
    "                # Parse JSON\n",
    "                df_clean = df_clean.withColumn('parsed_body', from_json(col('body_string'), json_schema))\n",
    "                \n",
    "                # Flatten parsed JSON fields\n",
    "                if 'parsed_body' in df_clean.columns:\n",
    "                    # Get all fields from parsed_body struct\n",
    "                    parsed_fields = df_clean.select('parsed_body.*').columns\n",
    "                    for field in parsed_fields:\n",
    "                        df_clean = df_clean.withColumn(f\"body_{field}\", col(f\"parsed_body.{field}\"))\n",
    "                    \n",
    "                    print(f\"    ‚úì Parsed Body JSON into {len(parsed_fields)} fields\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö† Could not parse JSON: {str(e)[:100]}\")\n",
    "    \n",
    "    final_count = df_clean.count()\n",
    "    retention_pct = (final_count / initial_count) * 100 if initial_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Initial: {initial_count:,}\")\n",
    "    print(f\"  Final: {final_count:,}\")\n",
    "    print(f\"  Removed: {initial_count - final_count:,}\")\n",
    "    print(f\"  Retention: {retention_pct:.1f}%\")\n",
    "    \n",
    "    # Store cleaned DataFrame\n",
    "    df_clean.createOrReplaceTempView(\"cleaned_eventhub_events\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Event Hub data cleaned and ready!\")\n",
    "    print(f\"\\nüìÑ Sample cleaned data:\")\n",
    "    display(df_clean.limit(5))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    print(\"Make sure Event Hub data has been copied to bronze layer (run Cell 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5e82fa-9841-41c2-be77-02ba1e535a9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VALIDATION: Verify cleaned data quality"
    }
   },
   "outputs": [],
   "source": [
    "# Validate cleaned data quality\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY VALIDATION - POST CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "# Validate MySQL tables\n",
    "if 'all_database_configs' in dir() and all_database_configs:\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        for table_name in tables:\n",
    "            view_name = f\"cleaned_{db_name}_{table_name}\"\n",
    "            \n",
    "            try:\n",
    "                df = spark.table(view_name)\n",
    "                \n",
    "                # Check for nulls in required fields\n",
    "                table_rules = cleaning_rules['table_specific'].get(table_name, {})\n",
    "                required_fields = table_rules.get('required_fields', [])\n",
    "                \n",
    "                null_in_required = False\n",
    "                for field in required_fields:\n",
    "                    if field in df.columns:\n",
    "                        null_count = df.filter(col(field).isNull()).count()\n",
    "                        if null_count > 0:\n",
    "                            null_in_required = True\n",
    "                            break\n",
    "                \n",
    "                # Check for duplicates\n",
    "                dedupe_key = table_rules.get('dedupe_key', [])\n",
    "                has_duplicates = False\n",
    "                if dedupe_key:\n",
    "                    total = df.count()\n",
    "                    unique = df.dropDuplicates(dedupe_key).count()\n",
    "                    has_duplicates = (total != unique)\n",
    "                \n",
    "                status = \"‚úÖ PASS\" if (not null_in_required and not has_duplicates) else \"‚ö†Ô∏è ISSUES\"\n",
    "                \n",
    "                validation_results.append({\n",
    "                    'table': f\"{db_name}.{table_name}\",\n",
    "                    'status': status,\n",
    "                    'null_in_required': null_in_required,\n",
    "                    'has_duplicates': has_duplicates,\n",
    "                    'record_count': df.count()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                validation_results.append({\n",
    "                    'table': f\"{db_name}.{table_name}\",\n",
    "                    'status': \"‚ùå ERROR\",\n",
    "                    'error': str(e)[:50]\n",
    "                })\n",
    "\n",
    "# Validate Event Hub\n",
    "try:\n",
    "    df_eh = spark.table(\"cleaned_eventhub_events\")\n",
    "    \n",
    "    null_in_required = df_eh.filter(\n",
    "        col('SequenceNumber').isNull() | col('Offset').isNull()\n",
    "    ).count() > 0\n",
    "    \n",
    "    has_duplicates = df_eh.count() != df_eh.dropDuplicates(['SequenceNumber', 'Offset']).count()\n",
    "    \n",
    "    status = \"‚úÖ PASS\" if (not null_in_required and not has_duplicates) else \"‚ö†Ô∏è ISSUES\"\n",
    "    \n",
    "    validation_results.append({\n",
    "        'table': 'eventhub.eventhub_events',\n",
    "        'status': status,\n",
    "        'null_in_required': null_in_required,\n",
    "        'has_duplicates': has_duplicates,\n",
    "        'record_count': df_eh.count()\n",
    "    })\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìã Validation Results:\\n\")\n",
    "for result in validation_results:\n",
    "    print(f\"{result['status']} {result['table']}\")\n",
    "    if 'record_count' in result:\n",
    "        print(f\"    Records: {result['record_count']:,}\")\n",
    "    if result.get('null_in_required'):\n",
    "        print(f\"    ‚ö† Has nulls in required fields\")\n",
    "    if result.get('has_duplicates'):\n",
    "        print(f\"    ‚ö† Has duplicate records\")\n",
    "    if 'error' in result:\n",
    "        print(f\"    ‚ùå Error: {result['error']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "passed = sum(1 for r in validation_results if r['status'] == \"‚úÖ PASS\")\n",
    "total = len(validation_results)\n",
    "print(f\"Validation: {passed}/{total} tables passed\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7be0a9-a53f-4551-bf22-dd9dad4c6136",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: MySQL Cleaned Data to Silver"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRODUCTION: Write CLEANED MySQL data to Silver Layer\n",
    "# This replaces the old Cell 19 - now uses cleaned data\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MYSQL CLEANED DATA TO SILVER PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå ERROR: Please run Cell 6 first!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found configuration for {len(all_database_configs)} database(s)\\n\")\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üìä Processing: {db_name}/{table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read CLEANED data from temp view\n",
    "                view_name = f\"cleaned_{db_name}_{table_name}\"\n",
    "                df_clean = spark.table(view_name)\n",
    "                \n",
    "                # Add processing metadata\n",
    "                df_silver = (df_clean\n",
    "                    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                    .withColumn(\"source_table\", lit(table_name))\n",
    "                    .withColumn(\"source_database\", lit(db_name))\n",
    "                )\n",
    "                \n",
    "                # Write to silver layer (overwrite mode for cleaned data)\n",
    "                df_silver.write\\\n",
    "                    .format(\"delta\")\\\n",
    "                    .mode(\"overwrite\")\\\n",
    "                    .option(\"mergeSchema\", \"true\")\\\n",
    "                    .save(f\"{silver_path}{table_name}/\")\n",
    "                \n",
    "                record_count = df_silver.count()\n",
    "                print(f\"     ‚úÖ Written {record_count:,} cleaned records to silver\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Completed database: {db_name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ MySQL Cleaned Data Pipeline Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852bd6ec-34c5-423f-af72-d3eac58ad230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Run this daily (uses Cell 1 config)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DAILY PRODUCTION PIPELINE\n",
    "# Processes all databases and tables discovered in Cell 1\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use the all_database_configs from Cell 1\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå ERROR: Please run Cell 1 first to discover databases and tables!\")\n",
    "    print(\"Cell 1 creates the 'all_database_configs' variable needed for processing.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found configuration for {len(all_database_configs)} database(s)\")\n",
    "    total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "    print(f\"Total tables to process: {total_tables}\\n\")\n",
    "    \n",
    "    # Process each database\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        silver_path = config['silver_path']\n",
    "        checkpoint_path = config['checkpoint_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"Tables: {len(tables)}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Process each table in this database\n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üìä Processing: {db_name}/{table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read with Auto Loader (only processes new files)\n",
    "                df_stream = (spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"parquet\")\n",
    "                    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}{table_name}/schema\")\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                    .option(\"recursiveFileLookup\", \"true\")\n",
    "                    .load(f\"{bronze_path}{table_name}/\")\n",
    "                )\n",
    "                \n",
    "                # Add metadata columns\n",
    "                df_enriched = (df_stream\n",
    "                    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                    .withColumn(\"source_table\", lit(table_name))\n",
    "                    .withColumn(\"source_database\", lit(db_name))\n",
    "                )\n",
    "                \n",
    "                # Write to silver layer\n",
    "                query = (df_enriched.writeStream\n",
    "                    .format(\"delta\")\n",
    "                    .option(\"checkpointLocation\", f\"{checkpoint_path}{table_name}/checkpoint\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .trigger(availableNow=True)\n",
    "                    .start(f\"{silver_path}{table_name}/\")\n",
    "                )\n",
    "                \n",
    "                # Wait for completion\n",
    "                query.awaitTermination()\n",
    "                \n",
    "                print(f\"     ‚úÖ Successfully processed {table_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error processing {table_name}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Completed database: {db_name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Daily Pipeline Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0baa3e1-a906-4daa-bc69-cb58782d6fd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Event Hub - Copy new data to Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVENT HUB TO BRONZE PIPELINE\n",
    "# Run this to copy new Event Hub data to bronze layer\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit, to_date\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB TO BRONZE PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "eventhub_source = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/streamingingestionsathya/eventhub/\"\n",
    "eventhub_name = \"eventhub_events\"\n",
    "\n",
    "# Bronze destination\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "checkpoint_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/eventhub_to_bronze/{eventhub_name}/\"\n",
    "\n",
    "print(f\"\\nSource: {eventhub_source}\")\n",
    "print(f\"Destination: {bronze_eventhub_path}\")\n",
    "print(f\"\\n‚è≥ Processing Event Hub data...\\n\")\n",
    "\n",
    "try:\n",
    "    # Read Event Hub data with Auto Loader\n",
    "    df_eventhub = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"avro\")  # Event Hub Capture uses AVRO\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_eventhub_path}schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(eventhub_source)\n",
    "    )\n",
    "    \n",
    "    # Add metadata and partition by date\n",
    "    df_enriched = (df_eventhub\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_system\", lit(\"eventhub\"))\n",
    "        .withColumn(\"load_date\", to_date(current_timestamp()))\n",
    "    )\n",
    "    \n",
    "    # Write to bronze layer\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_eventhub_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"load_date\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(bronze_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Event Hub data copied to Bronze layer!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify\n",
    "    df_bronze = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    record_count = df_bronze.count()\n",
    "    \n",
    "    print(f\"\\nTotal records in bronze: {record_count:,}\")\n",
    "    \n",
    "    if \"load_date\" in df_bronze.columns:\n",
    "        dates = df_bronze.select(\"load_date\").distinct().count()\n",
    "        print(f\"Distinct load dates: {dates}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)[:100]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0acde42-0edb-4c27-aa5c-5f64b1992910",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Event Hub - Process Bronze to Silver"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVENT HUB BRONZE TO SILVER PIPELINE\n",
    "# Run this after copying Event Hub data to bronze\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB BRONZE TO SILVER PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "eventhub_name = \"eventhub_events\"\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\n",
    "checkpoint_silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/eventhub/{eventhub_name}/\"\n",
    "\n",
    "print(f\"\\nBronze: {bronze_eventhub_path}\")\n",
    "print(f\"Silver: {silver_eventhub_path}\")\n",
    "print(f\"\\n‚è≥ Processing Event Hub bronze to silver...\\n\")\n",
    "\n",
    "try:\n",
    "    # Read from bronze layer\n",
    "    df_bronze = (spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .load(bronze_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    # Add processing metadata\n",
    "    df_enriched = (df_bronze\n",
    "        .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_table\", lit(eventhub_name))\n",
    "        .withColumn(\"source_database\", lit(\"eventhub\"))\n",
    "    )\n",
    "    \n",
    "    # Write to silver layer\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_silver_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(silver_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Event Hub data processed to Silver layer!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify\n",
    "    df_silver = spark.read.format(\"delta\").load(silver_eventhub_path)\n",
    "    record_count = df_silver.count()\n",
    "    \n",
    "    print(f\"\\nTotal records in silver: {record_count:,}\")\n",
    "    \n",
    "    if \"load_date\" in df_silver.columns:\n",
    "        dates = df_silver.select(\"load_date\").distinct().count()\n",
    "        print(f\"Distinct load dates: {dates}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)[:100]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eab700e-2e7c-4c9b-af17-34d3c0708bd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VERIFICATION: Complete Silver Layer Summary"
    }
   },
   "outputs": [],
   "source": [
    "# Complete verification of all data sources in silver layer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE SILVER LAYER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_sources = 0\n",
    "total_tables = 0\n",
    "total_records = 0\n",
    "\n",
    "# 1. MySQL Databases\n",
    "print(\"\\nüìä MYSQL DATABASES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' in dir() and all_database_configs:\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n  Database: {db_name}\")\n",
    "        \n",
    "        db_records = 0\n",
    "        for table_name in tables:\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(f\"{silver_path}{table_name}/\")\n",
    "                count = df.count()\n",
    "                db_records += count\n",
    "                print(f\"    ‚úì {table_name}: {count:,} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚úó {table_name}: Not processed\")\n",
    "        \n",
    "        print(f\"  Subtotal: {db_records:,} records\")\n",
    "        total_sources += 1\n",
    "        total_tables += len(tables)\n",
    "        total_records += db_records\n",
    "else:\n",
    "    print(\"  ‚ö† No MySQL databases configured\")\n",
    "\n",
    "# 2. Event Hub\n",
    "print(\"\\n\\nüì° EVENT HUB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "eventhub_name = \"eventhub_events\"\n",
    "silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\n",
    "\n",
    "try:\n",
    "    df_eh = spark.read.format(\"delta\").load(silver_eventhub_path)\n",
    "    eh_count = df_eh.count()\n",
    "    \n",
    "    print(f\"\\n  Source: eventhub\")\n",
    "    print(f\"    ‚úì {eventhub_name}: {eh_count:,} records\")\n",
    "    \n",
    "    if \"load_date\" in df_eh.columns:\n",
    "        dates = df_eh.select(\"load_date\").distinct().count()\n",
    "        print(f\"    ‚úì Load dates: {dates}\")\n",
    "    \n",
    "    print(f\"  Subtotal: {eh_count:,} records\")\n",
    "    total_sources += 1\n",
    "    total_tables += 1\n",
    "    total_records += eh_count\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n  ‚úó Event Hub: Not processed or error\")\n",
    "    print(f\"     {str(e)[:80]}\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Total Sources: {total_sources}\")\n",
    "print(f\"  Total Tables/Streams: {total_tables}\")\n",
    "print(f\"  Total Records: {total_records:,}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "if total_records > 0:\n",
    "    print(\"\\n‚úÖ SUCCESS! All data sources processed to silver layer!\")\n",
    "    print(\"\\nüìç Silver Layer Locations:\")\n",
    "    print(f\"   - MySQL: abfss://{container}@{storage_account}.dfs.core.windows.net/silver/mysql/\")\n",
    "    print(f\"   - Event Hub: abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No data found in silver layer. Please run the processing pipelines first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a39c53cd-2a7d-48a0-847b-46056f4565b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\r\n",
    "# ‚ö° **Delta Lake Optimization**\r\n",
    "\r\n",
    "## **Why Optimize?**\r\n",
    "\r\n",
    "* **Query Performance** - Up to 10-100x faster\r\n",
    "* **Storage Efficiency** - 30-50% reduction\r\n",
    "* **Cost Savings** - Less storage + faster queries\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Techniques**\r\n",
    "\r\n",
    "### **1. OPTIMIZE** - File Compaction\r\n",
    "* Combines small files into larger files\r\n",
    "* Run weekly after incremental loads\r\n",
    "\r\n",
    "### **2. Z-ORDER** - Data Clustering  \r\n",
    "* Co-locates related data\r\n",
    "* Enables data skipping\r\n",
    "* Best for 2-4 columns\r\n",
    "\r\n",
    "### **3. Liquid Clustering** ‚≠ê RECOMMENDED\r\n",
    "* Automatic incremental clustering\r\n",
    "* Self-optimizing\r\n",
    "* Best for 3-5 columns\r\n",
    "\r\n",
    "### **4. Optimized Writes**\r\n",
    "* Auto-compacts during writes\r\n",
    "* Enable at session start\r\n",
    "\r\n",
    "### **5. VACUUM**\r\n",
    "* Removes old file versions\r\n",
    "* Run monthly\r\n",
    "* Saves 20-50% storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b46f8c74-a7f6-4091-a43c-da29aeac063b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OPTIMIZE: Compact files"
    }
   },
   "outputs": [],
   "source": [
    "# Compact small files in Silver layer\r\n",
    "\r\n",
    "print(\"=\" * 70)\r\n",
    "print(\"OPTIMIZING SILVER LAYER - FILE COMPACTION\")\r\n",
    "print(\"=\" * 70)\r\n",
    "\r\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\r\n",
    "    print(\"\\n‚ùå Please run Cell 2 (Configuration) first!\")\r\n",
    "else:\r\n",
    "    # Optimize MySQL tables\r\n",
    "    for config in all_database_configs:\r\n",
    "        db_name = config['database_name']\r\n",
    "        silver_path = config['silver_path']\r\n",
    "        tables = config['tables']\r\n",
    "        \r\n",
    "        print(f\"\\nDatabase: {db_name}\")\r\n",
    "        \r\n",
    "        for table_name in tables:\r\n",
    "            try:\r\n",
    "                table_path = f\"{silver_path}{table_name}/\"\r\n",
    "                spark.sql(f\"OPTIMIZE delta.`{table_path}`\")\r\n",
    "                print(f\"  ‚úÖ {table_name}: Optimized\")\r\n",
    "            except Exception as e:\r\n",
    "                print(f\"  ‚ùå {table_name}: {str(e)[:80]}\")\r\n",
    "    \r\n",
    "    # Optimize Event Hub\r\n",
    "    eventhub_name = \"eventhub_events\"\r\n",
    "    silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\r\n",
    "    \r\n",
    "    try:\r\n",
    "        spark.sql(f\"OPTIMIZE delta.`{silver_eventhub_path}`\")\r\n",
    "        print(f\"\\n  ‚úÖ eventhub/{eventhub_name}: Optimized\")\r\n",
    "    except Exception as e:\r\n",
    "        print(f\"\\n  ‚ùå eventhub/{eventhub_name}: {str(e)[:80]}\")\r\n",
    "    \r\n",
    "    print(f\"\\n{'='*70}\")\r\n",
    "    print(\"‚úÖ OPTIMIZE COMPLETE\")\r\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "009c9b5e-53b8-4ecd-ae73-c56925385ce3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Z-ORDER: Cluster data"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Z-ORDER clustering for query performance\r\n",
    "\r\n",
    "print(\"=\" * 70)\r\n",
    "print(\"APPLYING Z-ORDER CLUSTERING\")\r\n",
    "print(\"=\" * 70)\r\n",
    "\r\n",
    "# Z-ORDER columns for each table\r\n",
    "zorder_config = {\r\n",
    "    'customer_details': ['customer_id', 'load_date'],\r\n",
    "    'customer_orders': ['order_id', 'customer_id'],\r\n",
    "    'orders': ['order_id', 'user_id'],\r\n",
    "    'users': ['user_id', 'country'],\r\n",
    "    'monthly_active_users': ['year__of_month'],\r\n",
    "    'departments': ['department_id'],\r\n",
    "    'employees': ['employee_id', 'department_id'],\r\n",
    "    'students_table': ['id', 'grade'],\r\n",
    "    'eventhub_events': ['load_date', 'SequenceNumber']\r\n",
    "}\r\n",
    "\r\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\r\n",
    "    print(\"\\n‚ùå Please run Cell 2 first!\")\r\n",
    "else:\r\n",
    "    # Z-ORDER MySQL tables\r\n",
    "    for config in all_database_configs:\r\n",
    "        db_name = config['database_name']\r\n",
    "        silver_path = config['silver_path']\r\n",
    "        tables = config['tables']\r\n",
    "        \r\n",
    "        print(f\"\\nDatabase: {db_name}\")\r\n",
    "        \r\n",
    "        for table_name in tables:\r\n",
    "            if table_name in zorder_config:\r\n",
    "                zorder_cols = zorder_config[table_name]\r\n",
    "                try:\r\n",
    "                    table_path = f\"{silver_path}{table_name}/\"\r\n",
    "                    zorder_clause = ', '.join(zorder_cols)\r\n",
    "                    spark.sql(f\"OPTIMIZE delta.`{table_path}` ZORDER BY ({zorder_clause})\")\r\n",
    "                    print(f\"  ‚úÖ {table_name}: Z-ORDERed by {', '.join(zorder_cols)}\")\r\n",
    "                except Exception as e:\r\n",
    "                    print(f\"  ‚ùå {table_name}: {str(e)[:80]}\")\r\n",
    "    \r\n",
    "    # Z-ORDER Event Hub\r\n",
    "    eventhub_name = \"eventhub_events\"\r\n",
    "    if eventhub_name in zorder_config:\r\n",
    "        zorder_cols = zorder_config[eventhub_name]\r\n",
    "        silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\r\n",
    "        try:\r\n",
    "            zorder_clause = ', '.join(zorder_cols)\r\n",
    "            spark.sql(f\"OPTIMIZE delta.`{silver_eventhub_path}` ZORDER BY ({zorder_clause})\")\r\n",
    "            print(f\"\\n  ‚úÖ eventhub/{eventhub_name}: Z-ORDERed by {', '.join(zorder_cols)}\")\r\n",
    "        except Exception as e:\r\n",
    "            print(f\"\\n  ‚ùå eventhub/{eventhub_name}: {str(e)[:80]}\")\r\n",
    "    \r\n",
    "    print(f\"\\n{'='*70}\")\r\n",
    "    print(\"‚úÖ Z-ORDER COMPLETE\")\r\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "635cbdcd-9d05-47d3-98c8-e3bfd382d02a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LIQUID CLUSTERING: Enable"
    }
   },
   "outputs": [],
   "source": [
    "# Enable Liquid Clustering (DBR 13.3+)\r\n",
    "\r\n",
    "print(\"=\" * 70)\r\n",
    "print(\"ENABLING LIQUID CLUSTERING\")\r\n",
    "print(\"=\" * 70)\r\n",
    "\r\n",
    "clustering_config = {\r\n",
    "    'customer_details': ['customer_id', 'load_date'],\r\n",
    "    'customer_orders': ['customer_id', 'order_id', 'load_date'],\r\n",
    "    'orders': ['user_id', 'order_date', 'load_date'],\r\n",
    "    'users': ['user_id', 'country', 'load_date'],\r\n",
    "    'monthly_active_users': ['year__of_month', 'load_date'],\r\n",
    "    'departments': ['department_id', 'load_date'],\r\n",
    "    'employees': ['department_id', 'employee_id', 'load_date'],\r\n",
    "    'students_table': ['grade', 'id', 'load_date'],\r\n",
    "    'eventhub_events': ['load_date', 'SequenceNumber']\r\n",
    "}\r\n",
    "\r\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\r\n",
    "    print(\"\\n‚ùå Please run Cell 2 first!\")\r\n",
    "else:\r\n",
    "    for config in all_database_configs:\r\n",
    "        db_name = config['database_name']\r\n",
    "        silver_path = config['silver_path']\r\n",
    "        tables = config['tables']\r\n",
    "        \r\n",
    "        print(f\"\\nDatabase: {db_name}\")\r\n",
    "        \r\n",
    "        for table_name in tables:\r\n",
    "            if table_name in clustering_config:\r\n",
    "                cluster_cols = clustering_config[table_name]\r\n",
    "                try:\r\n",
    "                    table_path = f\"{silver_path}{table_name}/\"\r\n",
    "                    cluster_clause = ', '.join(cluster_cols)\r\n",
    "                    spark.sql(f\"ALTER TABLE delta.`{table_path}` CLUSTER BY ({cluster_clause})\")\r\n",
    "                    print(f\"  ‚úÖ {table_name}: Liquid Clustering enabled\")\r\n",
    "                except Exception as e:\r\n",
    "                    if \"CLUSTER BY is only supported\" in str(e):\r\n",
    "                        print(f\"  ‚ö†Ô∏è  {table_name}: Not supported (use Z-ORDER instead)\")\r\n",
    "                    else:\r\n",
    "                        print(f\"  ‚ùå {table_name}: {str(e)[:80]}\")\r\n",
    "    \r\n",
    "    # Event Hub\r\n",
    "    eventhub_name = \"eventhub_events\"\r\n",
    "    if eventhub_name in clustering_config:\r\n",
    "        cluster_cols = clustering_config[eventhub_name]\r\n",
    "        silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\r\n",
    "        try:\r\n",
    "            cluster_clause = ', '.join(cluster_cols)\r\n",
    "            spark.sql(f\"ALTER TABLE delta.`{silver_eventhub_path}` CLUSTER BY ({cluster_clause})\")\r\n",
    "            print(f\"\\n  ‚úÖ eventhub/{eventhub_name}: Liquid Clustering enabled\")\r\n",
    "        except Exception as e:\r\n",
    "            if \"CLUSTER BY is only supported\" in str(e):\r\n",
    "                print(f\"\\n  ‚ö†Ô∏è  eventhub/{eventhub_name}: Not supported (use Z-ORDER)\")\r\n",
    "            else:\r\n",
    "                print(f\"\\n  ‚ùå eventhub/{eventhub_name}: {str(e)[:80]}\")\r\n",
    "    \r\n",
    "    print(f\"\\n{'='*70}\")\r\n",
    "    print(\"‚úÖ LIQUID CLUSTERING COMPLETE\")\r\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e7ec270-9a73-4c2b-bc00-1fc40d7f5fa7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OPTIMIZED WRITES: Enable"
    }
   },
   "outputs": [],
   "source": [
    "# Enable Optimized Writes for better write performance\r\n",
    "\r\n",
    "print(\"=\" * 70)\r\n",
    "print(\"ENABLING OPTIMIZED WRITES\")\r\n",
    "print(\"=\" * 70)\r\n",
    "\r\n",
    "# Enable optimized writes\r\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\r\n",
    "print(\"‚úÖ spark.databricks.delta.optimizeWrite.enabled = true\")\r\n",
    "\r\n",
    "# Enable auto compaction\r\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\r\n",
    "print(\"‚úÖ spark.databricks.delta.autoCompact.enabled = true\")\r\n",
    "\r\n",
    "print(\"\\n\" + \"=\" * 70)\r\n",
    "print(\"‚úÖ OPTIMIZED WRITES ENABLED\")\r\n",
    "print(\"=\" * 70)\r\n",
    "print(\"\\nüí° Applies to all future writes in this session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f92f93-812f-498d-90a5-1fc784f2bd89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VACUUM: Clean old versions"
    }
   },
   "outputs": [],
   "source": [
    "# VACUUM removes old file versions (Run monthly)\r\n",
    "# WARNING: Permanently deletes old versions\r\n",
    "\r\n",
    "print(\"=\" * 70)\r\n",
    "print(\"VACUUM - CLEAN UP OLD FILE VERSIONS\")\r\n",
    "print(\"=\" * 70)\r\n",
    "\r\n",
    "print(\"\\n‚ö†Ô∏è  WARNING: Code is commented for safety\")\r\n",
    "print(\"   Uncomment to execute\\n\")\r\n",
    "\r\n",
    "retention_hours = 168  # 7 days\r\n",
    "\r\n",
    "# UNCOMMENT TO RUN:\r\n",
    "# if 'all_database_configs' in dir() and all_database_configs:\r\n",
    "#     for config in all_database_configs:\r\n",
    "#         db_name = config['database_name']\r\n",
    "#         silver_path = config['silver_path']\r\n",
    "#         for table_name in config['tables']:\r\n",
    "#             try:\r\n",
    "#                 table_path = f\"{silver_path}{table_name}/\"\r\n",
    "#                 spark.sql(f\"VACUUM delta.`{table_path}` RETAIN {retention_hours} HOURS\")\r\n",
    "#                 print(f\"  ‚úÖ {db_name}.{table_name}: Vacuumed\")\r\n",
    "#             except Exception as e:\r\n",
    "#                 print(f\"  ‚ùå {db_name}.{table_name}: {str(e)[:80]}\")\r\n",
    "#     \r\n",
    "#     # Event Hub\r\n",
    "#     eventhub_name = \"eventhub_events\"\r\n",
    "#     silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\r\n",
    "#     try:\r\n",
    "#         spark.sql(f\"VACUUM delta.`{silver_eventhub_path}` RETAIN {retention_hours} HOURS\")\r\n",
    "#         print(f\"\\n  ‚úÖ eventhub/{eventhub_name}: Vacuumed\")\r\n",
    "#     except Exception as e:\r\n",
    "#         print(f\"\\n  ‚ùå eventhub/{eventhub_name}: {str(e)[:80]}\")\r\n",
    "\r\n",
    "print(\"\\nüí° Uncomment code above to run VACUUM\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Optimized_Bronze_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
