{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00e704b-a60a-4802-b00a-a675a46c1815",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration - Define paths and settings"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULLY DYNAMIC CONFIGURATION\n",
    "# Auto-discovers databases AND tables\n",
    "# ============================================\n",
    "\n",
    "# Base storage account and container\n",
    "storage_account = \"datamigrationsathya\"\n",
    "container = \"datalake\"\n",
    "\n",
    "# Path structure configuration\n",
    "layer = \"bronze\"  # bronze, silver, gold\n",
    "source_system = \"mysql\"  # mysql, postgres, etc.\n",
    "\n",
    "# Base path for the source system\n",
    "source_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/{source_system}/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTO-DISCOVERING DATABASES AND TABLES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSource system: {source_system}\")\n",
    "print(f\"Base path: {source_base_path}\")\n",
    "\n",
    "# ============================================\n",
    "# AUTO-DISCOVER ALL DATABASES\n",
    "# ============================================\n",
    "\n",
    "all_database_configs = []\n",
    "\n",
    "try:\n",
    "    # List all database folders under bronze/mysql/\n",
    "    database_folders = dbutils.fs.ls(source_base_path)\n",
    "    \n",
    "    print(f\"\\nFound {len(database_folders)} database(s):\\n\")\n",
    "    \n",
    "    for db_folder in database_folders:\n",
    "        if db_folder.isDir():\n",
    "            database_name = db_folder.name.rstrip('/')\n",
    "            \n",
    "            # Construct paths for this database\n",
    "            bronze_path = f\"{source_base_path}{database_name}/\"\n",
    "            silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "            checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "            \n",
    "            # Auto-discover tables in this database\n",
    "            try:\n",
    "                table_folders = dbutils.fs.ls(bronze_path)\n",
    "                tables = [t.name.rstrip('/') for t in table_folders if t.isDir()]\n",
    "                \n",
    "                if tables:  # Only add if tables exist\n",
    "                    all_database_configs.append({\n",
    "                        \"database_name\": database_name,\n",
    "                        \"bronze_path\": bronze_path,\n",
    "                        \"silver_path\": silver_path,\n",
    "                        \"checkpoint_path\": checkpoint_path,\n",
    "                        \"tables\": tables\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  ‚úì {database_name}: {len(tables)} table(s)\")\n",
    "                    for table in tables:\n",
    "                        print(f\"      - {table}\")\n",
    "                    print()\n",
    "                else:\n",
    "                    print(f\"  ‚ö† {database_name}: No tables found (skipping)\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {database_name}: Error reading tables - {str(e)}\\n\")\n",
    "                continue\n",
    "    \n",
    "    if not all_database_configs:\n",
    "        print(\"\\n‚ö†Ô∏è  No databases with tables found!\")\n",
    "        print(\"Please check your bronze layer structure.\")\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"SUMMARY: {len(all_database_configs)} database(s) ready to process\")\n",
    "        total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "        print(f\"Total tables across all databases: {total_tables}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error discovering databases: {str(e)}\")\n",
    "    print(\"\\nFalling back to manual configuration...\")\n",
    "    \n",
    "    # Fallback: Manual configuration\n",
    "    database_name = \"retail_db\"\n",
    "    bronze_base_path = f\"{source_base_path}{database_name}/\"\n",
    "    silver_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "    checkpoint_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "    \n",
    "    # Auto-discover tables\n",
    "    try:\n",
    "        folders = dbutils.fs.ls(bronze_base_path)\n",
    "        tables_to_process = [folder.name.rstrip('/') for folder in folders if folder.isDir()]\n",
    "        print(f\"Using manual database: {database_name}\")\n",
    "        print(f\"Found {len(tables_to_process)} tables: {tables_to_process}\")\n",
    "    except:\n",
    "        tables_to_process = [\"customer_details\"]\n",
    "        print(f\"Using fallback table list: {tables_to_process}\")\n",
    "    \n",
    "    # Create single database config for backward compatibility\n",
    "    all_database_configs = [{\n",
    "        \"database_name\": database_name,\n",
    "        \"bronze_path\": bronze_base_path,\n",
    "        \"silver_path\": silver_base_path,\n",
    "        \"checkpoint_path\": checkpoint_base_path,\n",
    "        \"tables\": tables_to_process\n",
    "    }]\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete!\")\n",
    "print(\"\\nNote: Auto Loader will recursively process all files in subdirectories (e.g., load_date partitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56dda7b7-77e0-44da-9a89-9522796ecbc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Advanced: Filter tables by pattern"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIONAL: Filter tables by pattern\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Process only specific tables\n",
    "include_tables = []  # Leave empty to process all, or specify: [\"customer_details\", \"orders\"]\n",
    "\n",
    "# Option 2: Exclude specific tables\n",
    "exclude_tables = []  # Example: [\"temp_table\", \"test_table\"]\n",
    "\n",
    "# Option 3: Filter by prefix/pattern\n",
    "table_prefix = \"\"  # Example: \"customer_\" to process only customer_* tables\n",
    "\n",
    "# Apply filters\n",
    "if include_tables:\n",
    "    tables_to_process = [t for t in tables_to_process if t in include_tables]\n",
    "    print(f\"\\nFiltered to include only: {include_tables}\")\n",
    "\n",
    "if exclude_tables:\n",
    "    tables_to_process = [t for t in tables_to_process if t not in exclude_tables]\n",
    "    print(f\"\\nExcluded tables: {exclude_tables}\")\n",
    "\n",
    "if table_prefix:\n",
    "    tables_to_process = [t for t in tables_to_process if t.startswith(table_prefix)]\n",
    "    print(f\"\\nFiltered by prefix '{table_prefix}'\")\n",
    "\n",
    "print(f\"\\nFinal tables to process ({len(tables_to_process)}):\")\n",
    "for table in tables_to_process:\n",
    "    print(f\"  ‚úì {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0acaa872-a7da-4463-9776-a51a6cb375e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Advanced: Handle multiple databases/schemas"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MULTI-DATABASE CONFIGURATION\n",
    "# Use this if you have multiple databases to process\n",
    "# ============================================\n",
    "\n",
    "# Define multiple database configurations\n",
    "database_configs = [\n",
    "    {\n",
    "        \"source_system\": \"mysql\",\n",
    "        \"database_name\": \"retail_db\",\n",
    "    },\n",
    "    # Uncomment and add more databases as needed:\n",
    "    # {\n",
    "    #     \"source_system\": \"mysql\",\n",
    "    #     \"database_name\": \"analytics_db\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"source_system\": \"postgres\",\n",
    "    #     \"database_name\": \"crm_db\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Build configuration for all databases\n",
    "all_configs = []\n",
    "\n",
    "for config in database_configs:\n",
    "    source = config[\"source_system\"]\n",
    "    db = config[\"database_name\"]\n",
    "    \n",
    "    bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/{source}/{db}/\"\n",
    "    silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source}/{db}/\"\n",
    "    checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source}/{db}/\"\n",
    "    \n",
    "    # Auto-discover tables for this database\n",
    "    try:\n",
    "        folders = dbutils.fs.ls(bronze_path)\n",
    "        tables = [folder.name.rstrip('/') for folder in folders if folder.isDir()]\n",
    "        \n",
    "        all_configs.append({\n",
    "            \"source_system\": source,\n",
    "            \"database_name\": db,\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"silver_path\": silver_path,\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \"tables\": tables\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚úì {source}/{db}: Found {len(tables)} tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó {source}/{db}: Error - {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total databases configured: {len(all_configs)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display summary\n",
    "for config in all_configs:\n",
    "    print(f\"\\n{config['source_system']}/{config['database_name']}:\")\n",
    "    print(f\"  Tables: {', '.join(config['tables'][:5])}{'...' if len(config['tables']) > 5 else ''}\")\n",
    "    print(f\"  Total: {len(config['tables'])} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686ec578-5849-4933-934e-8236a7b4426e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Date-Partitioned Folders\n",
    "\n",
    "Your bronze layer has a structure like:\n",
    "```\n",
    "customer_details/\n",
    "  ‚îú‚îÄ‚îÄ load_date=2026-01-31/\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file1.parquet\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file2.parquet\n",
    "  ‚îú‚îÄ‚îÄ load_date=2026-02-01/\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file1.parquet\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file2.parquet\n",
    "```\n",
    "\n",
    "**Good news:** Auto Loader (Options 1 & 4) automatically handles this!\n",
    "\n",
    "* **`recursiveFileLookup=true`** processes all files in all subdirectories\n",
    "* **Partition columns** (like `load_date`) are automatically extracted and added as columns\n",
    "* **New date folders** are automatically discovered and processed\n",
    "\n",
    "**For batch processing** (Options 2 & 3), Spark also automatically reads partition columns when you use the wildcard pattern or specify the parent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b18f657-e861-4679-b76f-e12c3708a367",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Verify partition columns are captured"
    }
   },
   "outputs": [],
   "source": [
    "# Run this to verify that partition columns (like load_date) are automatically captured\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read with partition discovery\n",
    "df_with_partitions = spark.read.parquet(f\"{bronze_base_path}{table_name}/\")\n",
    "\n",
    "print(\"Schema with partition columns:\")\n",
    "df_with_partitions.printSchema()\n",
    "\n",
    "print(\"\\nSample data showing load_date column:\")\n",
    "display(df_with_partitions.select(\"*\", \"load_date\").limit(10))\n",
    "\n",
    "print(\"\\nDistinct load dates in the data:\")\n",
    "df_with_partitions.select(\"load_date\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d274cf92-9dc4-478a-b7d8-c2aacd43ced7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 1: Process single table with Auto Loader"
    }
   },
   "outputs": [],
   "source": [
    "# Process a single table using Auto Loader (cloudFiles)\n",
    "# Auto Loader automatically discovers new files and processes them incrementally\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read all files from all subdirectories using Auto Loader\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")  # Change to \"json\", \"csv\", \"avro\" as needed\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")  # Process files in all subdirectories\n",
    "    .load(f\"{bronze_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Add metadata columns\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "df_enriched = (df\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_enriched.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aff6366c-c99c-4ead-8201-672b3fa2f194",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 2: Process all files with wildcard pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative: Read all parquet files across all folders at once\n",
    "# This is simpler but doesn't track which files have been processed\n",
    "\n",
    "# Use wildcard to read all parquet files in all subdirectories\n",
    "df_all = spark.read.parquet(f\"{bronze_base_path}*/**/\")\n",
    "\n",
    "print(f\"Total records: {df_all.count()}\")\n",
    "print(f\"Schema:\")\n",
    "df_all.printSchema()\n",
    "\n",
    "display(df_all.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ba63e1-7d52-4819-bf10-1e42dd5cff52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 3: Loop through multiple tables"
    }
   },
   "outputs": [],
   "source": [
    "# Process multiple tables in a loop\n",
    "# This approach processes each table separately\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data from bronze layer\n",
    "        df = spark.read.parquet(f\"{bronze_base_path}{table_name}/\")\n",
    "        \n",
    "        # Add processing metadata\n",
    "        from pyspark.sql.functions import current_timestamp, lit\n",
    "        df_processed = (df\n",
    "            .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "            .withColumn(\"source_table\", lit(table_name))\n",
    "        )\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"Records in {table_name}: {df_processed.count()}\")\n",
    "        \n",
    "        # Write to silver layer (optional - uncomment when ready)\n",
    "        # df_processed.write.mode(\"overwrite\").parquet(f\"{silver_base_path}{table_name}/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {table_name}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9a8bac-9dd0-4c07-b020-0b3037fe99c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 4: Write streaming data to silver layer"
    }
   },
   "outputs": [],
   "source": [
    "# Write the streaming data to silver layer using Auto Loader\n",
    "# This creates an incremental pipeline that processes new files automatically\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read with Auto Loader\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(f\"{bronze_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "df_enriched = (df_stream\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write to silver layer\n",
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")  # Use Delta format for silver layer\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_base_path}{table_name}/checkpoint\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)  # Process all available files then stop\n",
    "    .table(f\"silver.retail_db.{table_name}\")  # Or use .start(f\"{silver_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "query.awaitTermination()\n",
    "print(f\"Processing complete for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43622ffe-9527-4dfa-bbf1-cd486ccf8cfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper: List all folders and files"
    }
   },
   "outputs": [],
   "source": [
    "# Explore your bronze layer structure\n",
    "# This helps you understand what folders and files exist\n",
    "\n",
    "print(\"=== Bronze Layer Structure ===\")\n",
    "print(f\"\\nBase path: {bronze_base_path}\\n\")\n",
    "\n",
    "try:\n",
    "    # List all folders in bronze layer\n",
    "    folders = dbutils.fs.ls(bronze_base_path)\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder.isDir():\n",
    "            print(f\"üìÅ {folder.name}\")\n",
    "            \n",
    "            # List files in each folder (first level)\n",
    "            try:\n",
    "                subfolders = dbutils.fs.ls(folder.path)\n",
    "                for subfolder in subfolders[:5]:  # Show first 5 items\n",
    "                    print(f\"   ‚îî‚îÄ {subfolder.name} ({subfolder.size} bytes)\")\n",
    "                if len(subfolders) > 5:\n",
    "                    print(f\"   ‚îî‚îÄ ... and {len(subfolders) - 5} more items\")\n",
    "            except:\n",
    "                pass\n",
    "            print()\n",
    "except Exception as e:\n",
    "    print(f\"Error listing folders: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d983a11-7759-4eb9-a69f-0a4e88a3f381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## üìÖ Daily Production Run Guide\n",
    "\n",
    "### **Cells to Run Daily:**\n",
    "\n",
    "1. **Cell 1** - Configuration (always run first)\n",
    "2. **Cell 10** - Production pipeline (processes all tables incrementally)\n",
    "3. **Cell 11** - Verification (optional, to check results)\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "‚úÖ **First Run (Day 1):**\n",
    "- Processes ALL existing files in bronze layer\n",
    "- Creates checkpoints for each table\n",
    "- Writes data to silver layer\n",
    "\n",
    "‚úÖ **Subsequent Runs (Day 2, 3, 4...):**\n",
    "- **Only processes NEW files** added since last run\n",
    "- Skips already-processed files (based on checkpoint)\n",
    "- Completes in seconds if no new data\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Benefits:**\n",
    "\n",
    "* üöÄ **Efficient**: Only reads new data\n",
    "* üí∞ **Cost-effective**: Minimal compute usage\n",
    "* üîÑ **Idempotent**: Safe to run multiple times\n",
    "* üìÇ **Automatic**: Discovers new date partitions automatically\n",
    "* ‚ö° **Fast**: Completes quickly when no new data\n",
    "\n",
    "---\n",
    "\n",
    "### **Schedule Options:**\n",
    "\n",
    "**Option A - Manual:** Run Cell 1 ‚Üí Cell 10 daily\n",
    "\n",
    "**Option B - Databricks Job:** \n",
    "- Create a scheduled job that runs this notebook daily\n",
    "- Set schedule: Daily at specific time (e.g., 2 AM)\n",
    "- Job will automatically run cells 1 and 10\n",
    "\n",
    "**Option C - Workflow:**\n",
    "- Use Databricks Workflows for orchestration\n",
    "- Add dependencies if you have upstream processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38a9982-eb7d-48d6-863c-7c24a7d626af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Daily incremental pipeline for all tables"
    }
   },
   "outputs": [],
   "source": [
    "# This cell processes ALL tables incrementally using Auto Loader\n",
    "# Run this daily - it will only process NEW files added since last run\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Read with Auto Loader (only processes new files)\n",
    "        df_stream = (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .load(f\"{bronze_base_path}{table_name}/\")\n",
    "        )\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df_enriched = (df_stream\n",
    "            .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "            .withColumn(\"source_table\", lit(table_name))\n",
    "        )\n",
    "        \n",
    "        # Write to silver layer\n",
    "        query = (df_enriched.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", f\"{checkpoint_base_path}{table_name}/checkpoint\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(availableNow=True)  # Process all available files then stop\n",
    "            .start(f\"{silver_base_path}{table_name}/\")\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        print(f\"‚úÖ Successfully processed {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {table_name}: {str(e)}\")\n",
    "        # Continue with next table even if one fails\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Daily Pipeline Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4fadf7-7c59-4ea0-9a08-8153f75c72d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Multi-database daily pipeline"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRODUCTION PIPELINE - MULTI-DATABASE SUPPORT\n",
    "# This cell works with both single and multi-database configurations\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline (Multi-Database)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if multi-database config exists, otherwise use single database\n",
    "if 'all_configs' in locals() and all_configs:\n",
    "    configs_to_process = all_configs\n",
    "    print(f\"\\nProcessing {len(configs_to_process)} database(s)\")\n",
    "else:\n",
    "    # Single database mode\n",
    "    configs_to_process = [{\n",
    "        \"source_system\": source_system,\n",
    "        \"database_name\": database_name,\n",
    "        \"bronze_path\": bronze_base_path,\n",
    "        \"silver_path\": silver_base_path,\n",
    "        \"checkpoint_path\": checkpoint_base_path,\n",
    "        \"tables\": tables_to_process\n",
    "    }]\n",
    "    print(f\"\\nProcessing single database: {source_system}/{database_name}\")\n",
    "\n",
    "# Process each database\n",
    "for config in configs_to_process:\n",
    "    source = config['source_system']\n",
    "    db = config['database_name']\n",
    "    bronze_path = config['bronze_path']\n",
    "    silver_path = config['silver_path']\n",
    "    checkpoint_path = config['checkpoint_path']\n",
    "    tables = config['tables']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Database: {source}/{db}\")\n",
    "    print(f\"Tables to process: {len(tables)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Process each table in this database\n",
    "    for table_name in tables:\n",
    "        print(f\"\\n  Processing: {source}/{db}/{table_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read with Auto Loader (only processes new files)\n",
    "            df_stream = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"parquet\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}{table_name}/schema\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"recursiveFileLookup\", \"true\")\n",
    "                .load(f\"{bronze_path}{table_name}/\")\n",
    "            )\n",
    "            \n",
    "            # Add metadata columns\n",
    "            df_enriched = (df_stream\n",
    "                .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                .withColumn(\"source_file\", input_file_name())\n",
    "                .withColumn(\"source_table\", lit(table_name))\n",
    "                .withColumn(\"source_system\", lit(source))\n",
    "                .withColumn(\"source_database\", lit(db))\n",
    "            )\n",
    "            \n",
    "            # Write to silver layer\n",
    "            query = (df_enriched.writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", f\"{checkpoint_path}{table_name}/checkpoint\")\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .outputMode(\"append\")\n",
    "                .trigger(availableNow=True)\n",
    "                .start(f\"{silver_path}{table_name}/\")\n",
    "            )\n",
    "            \n",
    "            # Wait for completion\n",
    "            query.awaitTermination()\n",
    "            \n",
    "            print(f\"  ‚úÖ Successfully processed {table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {table_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed database: {source}/{db}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Daily Pipeline Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852bd6ec-34c5-423f-af72-d3eac58ad230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Run this daily (uses Cell 1 config)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DAILY PRODUCTION PIPELINE\n",
    "# Processes all databases and tables discovered in Cell 1\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use the all_database_configs from Cell 1\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå ERROR: Please run Cell 1 first to discover databases and tables!\")\n",
    "    print(\"Cell 1 creates the 'all_database_configs' variable needed for processing.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found configuration for {len(all_database_configs)} database(s)\")\n",
    "    total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "    print(f\"Total tables to process: {total_tables}\\n\")\n",
    "    \n",
    "    # Process each database\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        silver_path = config['silver_path']\n",
    "        checkpoint_path = config['checkpoint_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"Tables: {len(tables)}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Process each table in this database\n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üìä Processing: {db_name}/{table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read with Auto Loader (only processes new files)\n",
    "                df_stream = (spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"parquet\")\n",
    "                    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}{table_name}/schema\")\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                    .option(\"recursiveFileLookup\", \"true\")\n",
    "                    .load(f\"{bronze_path}{table_name}/\")\n",
    "                )\n",
    "                \n",
    "                # Add metadata columns\n",
    "                df_enriched = (df_stream\n",
    "                    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                    .withColumn(\"source_table\", lit(table_name))\n",
    "                    .withColumn(\"source_database\", lit(db_name))\n",
    "                )\n",
    "                \n",
    "                # Write to silver layer\n",
    "                query = (df_enriched.writeStream\n",
    "                    .format(\"delta\")\n",
    "                    .option(\"checkpointLocation\", f\"{checkpoint_path}{table_name}/checkpoint\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .trigger(availableNow=True)\n",
    "                    .start(f\"{silver_path}{table_name}/\")\n",
    "                )\n",
    "                \n",
    "                # Wait for completion\n",
    "                query.awaitTermination()\n",
    "                \n",
    "                print(f\"     ‚úÖ Successfully processed {table_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error processing {table_name}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Completed database: {db_name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Daily Pipeline Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5e6275-00e5-4832-8cc8-b121b5d6c9fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify: Check what was processed"
    }
   },
   "outputs": [],
   "source": [
    "# Verification: Check silver layer for all databases and tables\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 1 first!\")\n",
    "else:\n",
    "    total_processed = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\nüìä Database: {db_name}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for table_name in tables:\n",
    "            try:\n",
    "                # Read from silver layer\n",
    "                df = spark.read.format(\"delta\").load(f\"{silver_path}{table_name}/\")\n",
    "                count = df.count()\n",
    "                \n",
    "                # Get distinct load dates if column exists\n",
    "                if \"load_date\" in df.columns:\n",
    "                    dates = df.select(\"load_date\").distinct().count()\n",
    "                    print(f\"  ‚úì {table_name}: {count:,} records, {dates} load date(s)\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì {table_name}: {count:,} records\")\n",
    "                \n",
    "                total_processed += 1\n",
    "                total_records += count\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {table_name}: Not processed or error - {str(e)[:50]}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Databases: {len(all_database_configs)}\")\n",
    "    print(f\"Tables processed: {total_processed}\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92888d2-2dc9-42a5-b6e7-4b7aade7f915",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TEST: Process one table to verify setup"
    }
   },
   "outputs": [],
   "source": [
    "# Test processing a single table to verify the setup works\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "# Use first database and first table for testing\n",
    "test_config = all_database_configs[0]\n",
    "test_db = test_config['database_name']\n",
    "test_table = test_config['tables'][0]\n",
    "\n",
    "print(f\"üìä Testing with: {test_db}/{test_table}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bronze_path = f\"{test_config['bronze_path']}{test_table}/\"\n",
    "silver_path = f\"{test_config['silver_path']}{test_table}/\"\n",
    "checkpoint_path = f\"{test_config['checkpoint_path']}{test_table}/\"\n",
    "\n",
    "print(f\"\\nBronze: {bronze_path}\")\n",
    "print(f\"Silver: {silver_path}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# Check bronze layer has data\n",
    "print(f\"\\nüîç Checking bronze layer...\")\n",
    "try:\n",
    "    bronze_files = dbutils.fs.ls(bronze_path)\n",
    "    print(f\"   ‚úì Found {len(bronze_files)} item(s) in bronze layer\")\n",
    "    for item in bronze_files[:3]:\n",
    "        print(f\"     - {item.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Process with Auto Loader\n",
    "print(f\"\\n‚è≥ Processing with Auto Loader...\")\n",
    "\n",
    "try:\n",
    "    df_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(bronze_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Stream created\")\n",
    "    \n",
    "    # Add metadata\n",
    "    df_enriched = (df_stream\n",
    "        .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", input_file_name())\n",
    "        .withColumn(\"source_table\", lit(test_table))\n",
    "        .withColumn(\"source_database\", lit(test_db))\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Metadata columns added\")\n",
    "    \n",
    "    # Write to silver\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(silver_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Write stream started\")\n",
    "    print(f\"   ‚è≥ Waiting for completion...\")\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS! Processed {test_db}/{test_table}\")\n",
    "    \n",
    "    # Verify silver layer\n",
    "    print(f\"\\nüîç Verifying silver layer...\")\n",
    "    df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "    count = df_silver.count()\n",
    "    print(f\"   ‚úì Silver layer has {count:,} records\")\n",
    "    \n",
    "    print(f\"\\nüéâ Test successful! The pipeline is working correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38b161c-80e6-4260-94ed-adde8f45cbaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OPTIONAL: Verify silver layer data"
    }
   },
   "outputs": [],
   "source": [
    "# Run this after the daily pipeline to verify the results\n",
    "\n",
    "print(\"=== Silver Layer Summary ===\")\n",
    "print()\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(f\"{silver_base_path}{table_name}/\")\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Get distinct load dates\n",
    "        load_dates = df.select(\"load_date\").distinct().count()\n",
    "        \n",
    "        print(f\"üìä {table_name}:\")\n",
    "        print(f\"   Total records: {record_count:,}\")\n",
    "        print(f\"   Distinct load dates: {load_dates}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {table_name}: Not yet processed or error - {str(e)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59cb7c00-a5c7-4ca4-a4dc-5db5f43ce64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ö†Ô∏è Important: How Auto Loader Handles Modified Files\n",
    "\n",
    "### **Default Behavior:**\n",
    "Auto Loader tracks files by **path/name only**, not by content or modification time.\n",
    "\n",
    "**Scenario:**\n",
    "```\n",
    "load_date=2026-01-31/data_abc.parquet (processed on Day 1)\n",
    "load_date=2026-01-31/data_abc.parquet (modified on Day 2)\n",
    "```\n",
    "\n",
    "**Result:** Auto Loader will **SKIP** the modified file on Day 2 because the filename already exists in the checkpoint.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solutions:**\n",
    "\n",
    "**Option 1: Append-Only Pattern (Recommended)**\n",
    "- Never modify existing files\n",
    "- Always write NEW files with unique names (timestamps/UUIDs)\n",
    "- Example: `data_2026-01-31_v1.parquet`, `data_2026-01-31_v2.parquet`\n",
    "\n",
    "**Option 2: Use File Notifications (Azure Event Grid)**\n",
    "- Enable `cloudFiles.useNotifications=true`\n",
    "- Requires Azure Event Grid setup with proper IAM roles\n",
    "- Tracks file modifications via storage events\n",
    "\n",
    "**Option 3: Full Refresh Strategy**\n",
    "- Delete checkpoint and reprocess all files\n",
    "- Use for one-time fixes or major data corrections\n",
    "\n",
    "**Option 4: Partition-Level Reprocessing**\n",
    "- Delete specific partition data and checkpoint entries\n",
    "- Reprocess only affected partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d00c3a2-deee-483a-975a-4bd6d2c8087e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 1: Enable file notifications (detects modifications)"
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 1: Use Azure Event Grid to detect file modifications\n",
    "# This requires proper IAM roles on your storage account:\n",
    "# - Storage Account Contributor\n",
    "# - Storage Blob Data Contributor  \n",
    "# - EventGrid EventSubscription Contributor\n",
    "# - Storage Queue Data Contributor\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read with Auto Loader + File Notifications\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    \n",
    "    # Enable file notifications to detect modifications\n",
    "    .option(\"cloudFiles.useNotifications\", \"true\")  # Requires Event Grid setup\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")  # Process existing files on first run\n",
    "    \n",
    "    .load(f\"{bronze_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "df_enriched = (df_stream\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write to silver layer\n",
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_base_path}{table_name}/checkpoint_with_notifications\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(f\"{silver_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Processing complete for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1aa0e57-a713-4dd5-9a58-504e74d628a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 2: Full refresh - Delete checkpoint and reprocess"
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 2: Full refresh - Use when you need to reprocess ALL files\n",
    "# WARNING: This will reprocess ALL files, which can be expensive\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "print(f\"‚ö†Ô∏è  WARNING: This will delete checkpoint and reprocess ALL files for {table_name}\")\n",
    "print(f\"Checkpoint location: {checkpoint_base_path}{table_name}/\")\n",
    "\n",
    "# Uncomment the lines below to execute the full refresh\n",
    "# print(\"\\nDeleting checkpoint...\")\n",
    "# dbutils.fs.rm(f\"{checkpoint_base_path}{table_name}/\", recurse=True)\n",
    "# print(\"‚úÖ Checkpoint deleted\")\n",
    "\n",
    "# print(\"\\nDeleting existing silver layer data...\")\n",
    "# dbutils.fs.rm(f\"{silver_base_path}{table_name}/\", recurse=True)\n",
    "# print(\"‚úÖ Silver layer data deleted\")\n",
    "\n",
    "# print(\"\\nNow run Cell 10 to reprocess all files from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a33107-1430-45ea-800a-c079705f85f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 3: Partition-level reprocessing"
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 3: Reprocess specific partition (e.g., specific load_date)\n",
    "# Use this when only one date partition has modified files\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "partition_to_reprocess = \"2026-01-31\"  # Change this to the date you want to reprocess\n",
    "\n",
    "print(f\"Reprocessing partition: load_date={partition_to_reprocess}\")\n",
    "\n",
    "# Step 1: Delete data for this partition from silver layer\n",
    "print(f\"\\nStep 1: Deleting partition data from silver layer...\")\n",
    "try:\n",
    "    df_silver = spark.read.format(\"delta\").load(f\"{silver_base_path}{table_name}/\")\n",
    "    \n",
    "    # Delete records for this partition\n",
    "    df_filtered = df_silver.filter(col(\"load_date\") != partition_to_reprocess)\n",
    "    \n",
    "    # Overwrite silver layer (excluding the partition to reprocess)\n",
    "    df_filtered.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}{table_name}/\")\n",
    "    print(f\"‚úÖ Deleted partition data from silver layer\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error or no existing data: {str(e)}\")\n",
    "\n",
    "# Step 2: Delete checkpoint to force reprocessing\n",
    "print(f\"\\nStep 2: Deleting checkpoint...\")\n",
    "try:\n",
    "    dbutils.fs.rm(f\"{checkpoint_base_path}{table_name}/\", recurse=True)\n",
    "    print(f\"‚úÖ Checkpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error deleting checkpoint: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to reprocess. Run Cell 10 to process all files (including modified partition)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e344fffa-ee67-4872-a8a3-7529b857d9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Recommended Best Practice: Append-Only Pattern\n",
    "\n",
    "### **Instead of modifying files, use unique filenames:**\n",
    "\n",
    "**‚ùå Bad Pattern (causes issues):**\n",
    "```\n",
    "load_date=2026-01-31/data.parquet  (written on Day 1)\n",
    "load_date=2026-01-31/data.parquet  (overwritten on Day 2) ‚Üê Auto Loader misses this\n",
    "```\n",
    "\n",
    "**‚úÖ Good Pattern (works perfectly):**\n",
    "```\n",
    "load_date=2026-01-31/data_20260131_120000.parquet  (Day 1)\n",
    "load_date=2026-01-31/data_20260131_140000.parquet  (Day 2) ‚Üê Auto Loader picks this up\n",
    "```\n",
    "\n",
    "### **Implementation Tips:**\n",
    "\n",
    "1. **Add timestamps to filenames:**\n",
    "   ```python\n",
    "   from datetime import datetime\n",
    "   timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "   filename = f\"data_{timestamp}.parquet\"\n",
    "   ```\n",
    "\n",
    "2. **Use UUIDs for uniqueness:**\n",
    "   ```python\n",
    "   import uuid\n",
    "   filename = f\"data_{uuid.uuid4()}.parquet\"\n",
    "   ```\n",
    "\n",
    "3. **Configure upstream systems** to write with unique names\n",
    "\n",
    "4. **Never overwrite existing files** in bronze layer\n",
    "\n",
    "### **Benefits:**\n",
    "- Auto Loader works perfectly\n",
    "- Full audit trail of all data loads\n",
    "- Easy to track data lineage\n",
    "- No need for complex checkpoint management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abe9c733-0fe9-4e77-8def-be391e4a2697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
