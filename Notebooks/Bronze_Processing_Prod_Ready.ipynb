{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d252b6aa-bb3a-49cf-abd0-5c5044add3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULLY DYNAMIC CONFIGURATION\n",
    "# Auto-discovers databases AND tables\n",
    "# ============================================\n",
    "\n",
    "# Base storage account and container\n",
    "storage_account = \"datamigrationsathya\"\n",
    "container = \"datalake\"\n",
    "\n",
    "# Path structure configuration\n",
    "layer = \"bronze\"  # bronze, silver, gold\n",
    "source_system = \"mysql\"  # mysql, postgres, etc.\n",
    "\n",
    "# Base path for the source system\n",
    "source_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/{source_system}/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTO-DISCOVERING DATABASES AND TABLES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSource system: {source_system}\")\n",
    "print(f\"Base path: {source_base_path}\")\n",
    "\n",
    "# ============================================\n",
    "# AUTO-DISCOVER ALL DATABASES\n",
    "# ============================================\n",
    "\n",
    "all_database_configs = []\n",
    "\n",
    "try:\n",
    "    # List all database folders under bronze/mysql/\n",
    "    database_folders = dbutils.fs.ls(source_base_path)\n",
    "    \n",
    "    print(f\"\\nFound {len(database_folders)} database(s):\\n\")\n",
    "    \n",
    "    for db_folder in database_folders:\n",
    "        if db_folder.isDir():\n",
    "            database_name = db_folder.name.rstrip('/')\n",
    "            \n",
    "            # Construct paths for this database\n",
    "            bronze_path = f\"{source_base_path}{database_name}/\"\n",
    "            silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "            checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "            \n",
    "            # Auto-discover tables in this database\n",
    "            try:\n",
    "                table_folders = dbutils.fs.ls(bronze_path)\n",
    "                tables = [t.name.rstrip('/') for t in table_folders if t.isDir()]\n",
    "                \n",
    "                if tables:  # Only add if tables exist\n",
    "                    all_database_configs.append({\n",
    "                        \"database_name\": database_name,\n",
    "                        \"bronze_path\": bronze_path,\n",
    "                        \"silver_path\": silver_path,\n",
    "                        \"checkpoint_path\": checkpoint_path,\n",
    "                        \"tables\": tables\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  ‚úì {database_name}: {len(tables)} table(s)\")\n",
    "                    for table in tables:\n",
    "                        print(f\"      - {table}\")\n",
    "                    print()\n",
    "                else:\n",
    "                    print(f\"  ‚ö† {database_name}: No tables found (skipping)\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {database_name}: Error reading tables - {str(e)}\\n\")\n",
    "                continue\n",
    "    \n",
    "    if not all_database_configs:\n",
    "        print(\"\\n‚ö†Ô∏è  No databases with tables found!\")\n",
    "        print(\"Please check your bronze layer structure.\")\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"SUMMARY: {len(all_database_configs)} database(s) ready to process\")\n",
    "        total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "        print(f\"Total tables across all databases: {total_tables}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error discovering databases: {str(e)}\")\n",
    "    print(\"\\nFalling back to manual configuration...\")\n",
    "    \n",
    "    # Fallback: Manual configuration\n",
    "    database_name = \"retail_db\"\n",
    "    bronze_base_path = f\"{source_base_path}{database_name}/\"\n",
    "    silver_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "    checkpoint_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "    \n",
    "    # Auto-discover tables\n",
    "    try:\n",
    "        folders = dbutils.fs.ls(bronze_base_path)\n",
    "        tables_to_process = [folder.name.rstrip('/') for folder in folders if folder.isDir()]\n",
    "        print(f\"Using manual database: {database_name}\")\n",
    "        print(f\"Found {len(tables_to_process)} tables: {tables_to_process}\")\n",
    "    except:\n",
    "        tables_to_process = [\"customer_details\"]\n",
    "        print(f\"Using fallback table list: {tables_to_process}\")\n",
    "    \n",
    "    # Create single database config for backward compatibility\n",
    "    all_database_configs = [{\n",
    "        \"database_name\": database_name,\n",
    "        \"bronze_path\": bronze_base_path,\n",
    "        \"silver_path\": silver_base_path,\n",
    "        \"checkpoint_path\": checkpoint_base_path,\n",
    "        \"tables\": tables_to_process\n",
    "    }]\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete!\")\n",
    "print(\"\\nNote: Auto Loader will recursively process all files in subdirectories (e.g., load_date partitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83fd9101-25b0-405b-8cd6-6370e8633897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification: Check silver layer for all databases and tables\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 1 first!\")\n",
    "else:\n",
    "    total_processed = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\nüìä Database: {db_name}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for table_name in tables:\n",
    "            try:\n",
    "                # Read from silver layer\n",
    "                df = spark.read.format(\"delta\").load(f\"{silver_path}{table_name}/\")\n",
    "                count = df.count()\n",
    "                \n",
    "                # Get distinct load dates if column exists\n",
    "                if \"load_date\" in df.columns:\n",
    "                    dates = df.select(\"load_date\").distinct().count()\n",
    "                    print(f\"  ‚úì {table_name}: {count:,} records, {dates} load date(s)\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì {table_name}: {count:,} records\")\n",
    "                \n",
    "                total_processed += 1\n",
    "                total_records += count\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {table_name}: Not processed or error - {str(e)[:50]}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Databases: {len(all_database_configs)}\")\n",
    "    print(f\"Tables processed: {total_processed}\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33972827-2353-48d0-918c-521cc72f6566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Test processing a single table to verify the setup works\n",
    "\n",
    "# from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "# # Use first database and first table for testing\n",
    "# test_config = all_database_configs[0]\n",
    "# test_db = test_config['database_name']\n",
    "# test_table = test_config['tables'][0]\n",
    "\n",
    "# print(f\"üìä Testing with: {test_db}/{test_table}\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# bronze_path = f\"{test_config['bronze_path']}{test_table}/\"\n",
    "# silver_path = f\"{test_config['silver_path']}{test_table}/\"\n",
    "# checkpoint_path = f\"{test_config['checkpoint_path']}{test_table}/\"\n",
    "\n",
    "# print(f\"\\nBronze: {bronze_path}\")\n",
    "# print(f\"Silver: {silver_path}\")\n",
    "# print(f\"Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# # Check bronze layer has data\n",
    "# print(f\"\\nüîç Checking bronze layer...\")\n",
    "# try:\n",
    "#     bronze_files = dbutils.fs.ls(bronze_path)\n",
    "#     print(f\"   ‚úì Found {len(bronze_files)} item(s) in bronze layer\")\n",
    "#     for item in bronze_files[:3]:\n",
    "#         print(f\"     - {item.name}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"   ‚ùå Error: {str(e)}\")\n",
    "#     raise\n",
    "\n",
    "# # Process with Auto Loader\n",
    "# print(f\"\\n‚è≥ Processing with Auto Loader...\")\n",
    "\n",
    "# try:\n",
    "#     df_stream = (spark.readStream\n",
    "#         .format(\"cloudFiles\")\n",
    "#         .option(\"cloudFiles.format\", \"parquet\")\n",
    "#         .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}schema\")\n",
    "#         .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "#         .option(\"recursiveFileLookup\", \"true\")\n",
    "#         .load(bronze_path)\n",
    "#     )\n",
    "    \n",
    "#     print(f\"   ‚úì Stream created\")\n",
    "    \n",
    "#     # Add metadata\n",
    "#     df_enriched = (df_stream\n",
    "#         .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "#         .withColumn(\"source_file\", input_file_name())\n",
    "#         .withColumn(\"source_table\", lit(test_table))\n",
    "#         .withColumn(\"source_database\", lit(test_db))\n",
    "#     )\n",
    "    \n",
    "#     print(f\"   ‚úì Metadata columns added\")\n",
    "    \n",
    "#     # Write to silver\n",
    "#     query = (df_enriched.writeStream\n",
    "#         .format(\"delta\")\n",
    "#         .option(\"checkpointLocation\", f\"{checkpoint_path}checkpoint\")\n",
    "#         .option(\"mergeSchema\", \"true\")\n",
    "#         .outputMode(\"append\")\n",
    "#         .trigger(availableNow=True)\n",
    "#         .start(silver_path)\n",
    "#     )\n",
    "    \n",
    "#     print(f\"   ‚úì Write stream started\")\n",
    "#     print(f\"   ‚è≥ Waiting for completion...\")\n",
    "    \n",
    "#     query.awaitTermination()\n",
    "    \n",
    "#     print(f\"\\n‚úÖ SUCCESS! Processed {test_db}/{test_table}\")\n",
    "    \n",
    "#     # Verify silver layer\n",
    "#     print(f\"\\nüîç Verifying silver layer...\")\n",
    "#     df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "#     count = df_silver.count()\n",
    "#     print(f\"   ‚úì Silver layer has {count:,} records\")\n",
    "    \n",
    "#     print(f\"\\nüéâ Test successful! The pipeline is working correctly.\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Processing_Prod_Ready",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
