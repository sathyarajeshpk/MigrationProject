{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c8584f-0caf-4e1d-8fa1-376f517776b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Event Hub: Explore streaming data structure"
    }
   },
   "outputs": [],
   "source": [
    "# Explore Event Hub streaming data structure\n",
    "\n",
    "eventhub_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/streamingingestionsathya/eventhub/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB STREAMING DATA STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nBase path: {eventhub_base_path}\\n\")\n",
    "\n",
    "try:\n",
    "    # List partitions (0, 1, 2, etc.)\n",
    "    partitions = dbutils.fs.ls(eventhub_base_path)\n",
    "    \n",
    "    print(f\"Found {len(partitions)} partition(s):\\n\")\n",
    "    \n",
    "    for partition in partitions[:5]:  # Show first 5 partitions\n",
    "        if partition.isDir():\n",
    "            partition_name = partition.name.rstrip('/')\n",
    "            print(f\"  üìÅ Partition: {partition_name}\")\n",
    "            \n",
    "            # Sample the structure - look at year folders\n",
    "            try:\n",
    "                years = dbutils.fs.ls(partition.path)\n",
    "                year_names = [y.name.rstrip('/') for y in years if y.isDir()]\n",
    "                print(f\"     Years: {', '.join(year_names[:3])}{'...' if len(year_names) > 3 else ''}\")\n",
    "                \n",
    "                # Count total files in this partition (sample)\n",
    "                file_count = 0\n",
    "                for year in years[:1]:  # Check first year only\n",
    "                    try:\n",
    "                        # Recursively count files\n",
    "                        all_items = dbutils.fs.ls(year.path)\n",
    "                        for item in all_items:\n",
    "                            if not item.isDir():\n",
    "                                file_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if file_count > 0:\n",
    "                    print(f\"     Sample file count: ~{file_count} files\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"     Error exploring: {str(e)[:50]}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Structure: eventhub/{partition}/{year}/{month}/{day}/{hour}/{minute}/\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c395c3-d826-46ac-b815-24e8339c6475",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Event Hub: Sample data files"
    }
   },
   "outputs": [],
   "source": [
    "# Find and read sample Event Hub files to understand the data format\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLING EVENT HUB DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Find a sample file\n",
    "    sample_path = f\"{eventhub_base_path}0/2026/02/01/17/03/\"\n",
    "    \n",
    "    print(f\"\\nSample path: {sample_path}\\n\")\n",
    "    \n",
    "    files = dbutils.fs.ls(sample_path)\n",
    "    avro_files = [f for f in files if f.name.endswith('.avro')]\n",
    "    \n",
    "    if avro_files:\n",
    "        print(f\"Found {len(avro_files)} AVRO file(s)\")\n",
    "        print(f\"Sample file: {avro_files[0].name}\\n\")\n",
    "        \n",
    "        # Read sample AVRO file\n",
    "        print(\"Reading sample data...\\n\")\n",
    "        df_sample = spark.read.format(\"avro\").load(avro_files[0].path)\n",
    "        \n",
    "        print(\"Schema:\")\n",
    "        df_sample.printSchema()\n",
    "        \n",
    "        print(f\"\\nRecord count: {df_sample.count()}\")\n",
    "        \n",
    "        print(\"\\nSample records:\")\n",
    "        display(df_sample.limit(5))\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No AVRO files found in sample path\")\n",
    "        print(\"Listing all files:\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f.name}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    print(\"\\nTrying to find any available files...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to find files recursively\n",
    "        all_files = dbutils.fs.ls(eventhub_base_path + \"0/2026/\")\n",
    "        print(f\"\\nFound {len(all_files)} items in 2026 folder\")\n",
    "        for item in all_files[:10]:\n",
    "            print(f\"  - {item.name}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21f81c2-a34f-46ca-932c-1e04dc532b4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Event Hub: Copy to Bronze layer (Auto Loader approach)"
    }
   },
   "outputs": [],
   "source": [
    "# Copy Event Hub data to Bronze layer using Auto Loader\n",
    "# This approach processes files incrementally and handles new data automatically\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "\n",
    "# Configuration\n",
    "eventhub_source = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/streamingingestionsathya/eventhub/\"\n",
    "eventhub_name = \"eventhub_events\"  # Name for this event hub stream\n",
    "\n",
    "# Bronze destination\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/eventhub/{eventhub_name}/\"\n",
    "checkpoint_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/eventhub_to_bronze/{eventhub_name}/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COPYING EVENT HUB DATA TO BRONZE LAYER\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSource: {eventhub_source}\")\n",
    "print(f\"Destination: {bronze_eventhub_path}\")\n",
    "print(f\"Checkpoint: {checkpoint_eventhub_path}\\n\")\n",
    "\n",
    "try:\n",
    "    # Read Event Hub data with Auto Loader\n",
    "    df_eventhub = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"avro\")  # Event Hub Capture uses AVRO\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_eventhub_path}schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")  # Process all nested folders\n",
    "        .load(eventhub_source)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Stream created\")\n",
    "    \n",
    "    # Add metadata and partition by date\n",
    "    from pyspark.sql.functions import to_date\n",
    "    \n",
    "    df_enriched = (df_eventhub\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_system\", lit(\"eventhub\"))\n",
    "        .withColumn(\"load_date\", to_date(current_timestamp()))  # Partition by load date\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Metadata columns added\")\n",
    "    \n",
    "    # Write to bronze layer with partitioning\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_eventhub_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"load_date\")  # Partition by date for efficient processing\n",
    "        .trigger(availableNow=True)  # Process all available data then stop\n",
    "        .start(bronze_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Write stream started\")\n",
    "    print(\"‚è≥ Processing Event Hub data...\\n\")\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Event Hub data copied to Bronze layer!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify\n",
    "    df_bronze = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    record_count = df_bronze.count()\n",
    "    \n",
    "    print(f\"\\nTotal records in bronze: {record_count:,}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    display(df_bronze.limit(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00893e0a-1769-4ea0-be83-0f62af52b5e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Event Hub: Verify bronze layer data"
    }
   },
   "outputs": [],
   "source": [
    "# Verify Event Hub data in bronze layer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB BRONZE LAYER VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    \n",
    "    print(f\"\\nüìä Total records: {df_bronze.count():,}\")\n",
    "    \n",
    "    # Show partitions\n",
    "    if \"load_date\" in df_bronze.columns:\n",
    "        print(\"\\nüìÖ Load dates:\")\n",
    "        df_bronze.select(\"load_date\").distinct().orderBy(\"load_date\").show()\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nüìã Schema:\")\n",
    "    df_bronze.printSchema()\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nüìÑ Sample records:\")\n",
    "    display(df_bronze.limit(10))\n",
    "    \n",
    "    print(\"\\n‚úÖ Event Hub data is ready for processing!\")\n",
    "    print(f\"\\nBronze path: {bronze_eventhub_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    print(\"\\nMake sure you've run the previous cell to copy data to bronze layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7e9923-7028-49ab-8f7f-c1afd21eec65",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Event Hub: Add to main configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Add Event Hub to the main all_database_configs for unified processing\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ADDING EVENT HUB TO MAIN CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Add Event Hub as a new \"database\" in the configuration\n",
    "eventhub_config = {\n",
    "    \"database_name\": \"eventhub\",\n",
    "    \"bronze_path\": f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/eventhub/\",\n",
    "    \"silver_path\": f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/\",\n",
    "    \"checkpoint_path\": f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/eventhub/\",\n",
    "    \"tables\": [eventhub_name]  # eventhub_events\n",
    "}\n",
    "\n",
    "# Check if already exists\n",
    "existing_dbs = [config['database_name'] for config in all_database_configs]\n",
    "\n",
    "if 'eventhub' not in existing_dbs:\n",
    "    all_database_configs.append(eventhub_config)\n",
    "    print(\"\\n‚úÖ Event Hub added to configuration!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Event Hub already in configuration\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UPDATED CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for config in all_database_configs:\n",
    "    db_name = config['database_name']\n",
    "    tables = config['tables']\n",
    "    print(f\"\\nüìä {db_name}: {len(tables)} table(s)\")\n",
    "    for table in tables:\n",
    "        print(f\"    - {table}\")\n",
    "\n",
    "total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total: {len(all_database_configs)} source(s), {total_tables} table(s)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\n‚úÖ Now you can run the main production pipeline (Cell 15) to process ALL data!\")\n",
    "print(\"   It will process: MySQL databases + Event Hub data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00e704b-a60a-4802-b00a-a675a46c1815",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration - Define paths and settings"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULLY DYNAMIC CONFIGURATION\n",
    "# Auto-discovers databases AND tables\n",
    "# ============================================\n",
    "\n",
    "# Base storage account and container\n",
    "storage_account = \"datamigrationsathya\"\n",
    "container = \"datalake\"\n",
    "\n",
    "# Path structure configuration\n",
    "layer = \"bronze\"  # bronze, silver, gold\n",
    "source_system = \"mysql\"  # mysql, postgres, etc.\n",
    "\n",
    "# Base path for the source system\n",
    "source_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/{source_system}/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTO-DISCOVERING DATABASES AND TABLES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSource system: {source_system}\")\n",
    "print(f\"Base path: {source_base_path}\")\n",
    "\n",
    "# ============================================\n",
    "# AUTO-DISCOVER ALL DATABASES\n",
    "# ============================================\n",
    "\n",
    "all_database_configs = []\n",
    "\n",
    "try:\n",
    "    # List all database folders under bronze/mysql/\n",
    "    database_folders = dbutils.fs.ls(source_base_path)\n",
    "    \n",
    "    print(f\"\\nFound {len(database_folders)} database(s):\\n\")\n",
    "    \n",
    "    for db_folder in database_folders:\n",
    "        if db_folder.isDir():\n",
    "            database_name = db_folder.name.rstrip('/')\n",
    "            \n",
    "            # Construct paths for this database\n",
    "            bronze_path = f\"{source_base_path}{database_name}/\"\n",
    "            silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "            checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "            \n",
    "            # Auto-discover tables in this database\n",
    "            try:\n",
    "                table_folders = dbutils.fs.ls(bronze_path)\n",
    "                tables = [t.name.rstrip('/') for t in table_folders if t.isDir()]\n",
    "                \n",
    "                if tables:  # Only add if tables exist\n",
    "                    all_database_configs.append({\n",
    "                        \"database_name\": database_name,\n",
    "                        \"bronze_path\": bronze_path,\n",
    "                        \"silver_path\": silver_path,\n",
    "                        \"checkpoint_path\": checkpoint_path,\n",
    "                        \"tables\": tables\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  ‚úì {database_name}: {len(tables)} table(s)\")\n",
    "                    for table in tables:\n",
    "                        print(f\"      - {table}\")\n",
    "                    print()\n",
    "                else:\n",
    "                    print(f\"  ‚ö† {database_name}: No tables found (skipping)\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {database_name}: Error reading tables - {str(e)}\\n\")\n",
    "                continue\n",
    "    \n",
    "    if not all_database_configs:\n",
    "        print(\"\\n‚ö†Ô∏è  No databases with tables found!\")\n",
    "        print(\"Please check your bronze layer structure.\")\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"SUMMARY: {len(all_database_configs)} database(s) ready to process\")\n",
    "        total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "        print(f\"Total tables across all databases: {total_tables}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error discovering databases: {str(e)}\")\n",
    "    print(\"\\nFalling back to manual configuration...\")\n",
    "    \n",
    "    # Fallback: Manual configuration\n",
    "    database_name = \"retail_db\"\n",
    "    bronze_base_path = f\"{source_base_path}{database_name}/\"\n",
    "    silver_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source_system}/{database_name}/\"\n",
    "    checkpoint_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source_system}/{database_name}/\"\n",
    "    \n",
    "    # Auto-discover tables\n",
    "    try:\n",
    "        folders = dbutils.fs.ls(bronze_base_path)\n",
    "        tables_to_process = [folder.name.rstrip('/') for folder in folders if folder.isDir()]\n",
    "        print(f\"Using manual database: {database_name}\")\n",
    "        print(f\"Found {len(tables_to_process)} tables: {tables_to_process}\")\n",
    "    except:\n",
    "        tables_to_process = [\"customer_details\"]\n",
    "        print(f\"Using fallback table list: {tables_to_process}\")\n",
    "    \n",
    "    # Create single database config for backward compatibility\n",
    "    all_database_configs = [{\n",
    "        \"database_name\": database_name,\n",
    "        \"bronze_path\": bronze_base_path,\n",
    "        \"silver_path\": silver_base_path,\n",
    "        \"checkpoint_path\": checkpoint_base_path,\n",
    "        \"tables\": tables_to_process\n",
    "    }]\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete!\")\n",
    "print(\"\\nNote: Auto Loader will recursively process all files in subdirectories (e.g., load_date partitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57c5cb0-3bd8-430e-8bb2-efe87fbea5eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ANALYSIS: Analyze MySQL Bronze Data Quality"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive data quality analysis for MySQL bronze layer\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, sum as spark_sum, avg, min as spark_min, max as spark_max\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MYSQL BRONZE LAYER - DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 6 (Configuration) first!\")\n",
    "else:\n",
    "    quality_report = []\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for table_name in tables:\n",
    "            print(f\"\\nüìä Analyzing: {table_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            try:\n",
    "                # Read bronze data\n",
    "                df = spark.read.parquet(f\"{bronze_path}{table_name}/\")\n",
    "                \n",
    "                total_records = df.count()\n",
    "                total_columns = len(df.columns)\n",
    "                \n",
    "                print(f\"  Total Records: {total_records:,}\")\n",
    "                print(f\"  Total Columns: {total_columns}\")\n",
    "                \n",
    "                # 1. NULL ANALYSIS\n",
    "                print(f\"\\n  üìã Null Analysis:\")\n",
    "                null_counts = df.select([spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]).collect()[0]\n",
    "                \n",
    "                has_nulls = False\n",
    "                for col_name in df.columns:\n",
    "                    null_count = null_counts[col_name]\n",
    "                    if null_count > 0:\n",
    "                        null_pct = (null_count / total_records) * 100\n",
    "                        print(f\"    ‚ö† {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "                        has_nulls = True\n",
    "                \n",
    "                if not has_nulls:\n",
    "                    print(f\"    ‚úì No null values found\")\n",
    "                \n",
    "                # 2. DUPLICATE ANALYSIS\n",
    "                print(f\"\\n  üîÑ Duplicate Analysis:\")\n",
    "                duplicate_count = total_records - df.dropDuplicates().count()\n",
    "                if duplicate_count > 0:\n",
    "                    print(f\"    ‚ö† Found {duplicate_count:,} duplicate records ({(duplicate_count/total_records)*100:.1f}%)\")\n",
    "                else:\n",
    "                    print(f\"    ‚úì No duplicates found\")\n",
    "                \n",
    "                # 3. DATA TYPE ANALYSIS\n",
    "                print(f\"\\n  üìù Schema:\")\n",
    "                for field in df.schema.fields:\n",
    "                    print(f\"    - {field.name}: {field.dataType}\")\n",
    "                \n",
    "                # 4. SAMPLE DATA\n",
    "                print(f\"\\n  üìÑ Sample Records (first 3):\")\n",
    "                display(df.limit(3))\n",
    "                \n",
    "                # Store quality metrics\n",
    "                quality_report.append({\n",
    "                    'database': db_name,\n",
    "                    'table': table_name,\n",
    "                    'total_records': total_records,\n",
    "                    'total_columns': total_columns,\n",
    "                    'has_nulls': has_nulls,\n",
    "                    'duplicate_count': duplicate_count\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error analyzing {table_name}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Summary Report\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"QUALITY ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for report in quality_report:\n",
    "        status = \"‚ö† Issues Found\" if (report['has_nulls'] or report['duplicate_count'] > 0) else \"‚úì Clean\"\n",
    "        print(f\"\\n{report['database']}.{report['table']}: {status}\")\n",
    "        print(f\"  Records: {report['total_records']:,}\")\n",
    "        if report['has_nulls']:\n",
    "            print(f\"  ‚ö† Has null values\")\n",
    "        if report['duplicate_count'] > 0:\n",
    "            print(f\"  ‚ö† Has {report['duplicate_count']:,} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e10db409-0622-46c9-ba23-6d654c55e832",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ANALYSIS: Analyze Event Hub Bronze Data Quality"
    }
   },
   "outputs": [],
   "source": [
    "# Data quality analysis for Event Hub bronze layer\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, length, size\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB BRONZE LAYER - DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "eventhub_name = \"eventhub_events\"\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "\n",
    "try:\n",
    "    df_eh = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    \n",
    "    total_records = df_eh.count()\n",
    "    print(f\"\\nTotal Records: {total_records:,}\")\n",
    "    print(f\"Total Columns: {len(df_eh.columns)}\")\n",
    "    \n",
    "    # 1. NULL ANALYSIS\n",
    "    print(f\"\\nüìã Null Analysis:\")\n",
    "    null_counts = df_eh.select([spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_eh.columns]).collect()[0]\n",
    "    \n",
    "    for col_name in df_eh.columns:\n",
    "        null_count = null_counts[col_name]\n",
    "        if null_count > 0:\n",
    "            null_pct = (null_count / total_records) * 100\n",
    "            print(f\"  ‚ö† {col_name}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "    \n",
    "    # 2. BODY FIELD ANALYSIS (contains JSON data)\n",
    "    print(f\"\\nüì¶ Body Field Analysis:\")\n",
    "    if 'Body' in df_eh.columns:\n",
    "        # Check for empty bodies\n",
    "        empty_bodies = df_eh.filter(col('Body').isNull()).count()\n",
    "        print(f\"  Empty bodies: {empty_bodies}\")\n",
    "        \n",
    "        # Sample body content\n",
    "        print(f\"\\n  Sample Body content (first record):\")\n",
    "        sample_body = df_eh.select('Body').limit(1).collect()[0]['Body']\n",
    "        if sample_body:\n",
    "            import base64\n",
    "            decoded = base64.b64decode(sample_body).decode('utf-8')\n",
    "            print(f\"  {decoded[:200]}...\")\n",
    "    \n",
    "    # 3. DUPLICATE ANALYSIS\n",
    "    print(f\"\\nüîÑ Duplicate Analysis:\")\n",
    "    duplicate_count = total_records - df_eh.dropDuplicates(['SequenceNumber', 'Offset']).count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"  ‚ö† Found {duplicate_count:,} duplicate records\")\n",
    "    else:\n",
    "        print(f\"  ‚úì No duplicates found\")\n",
    "    \n",
    "    # 4. LOAD DATE DISTRIBUTION\n",
    "    print(f\"\\nüìÖ Load Date Distribution:\")\n",
    "    if 'load_date' in df_eh.columns:\n",
    "        df_eh.groupBy('load_date').count().orderBy('load_date').show()\n",
    "    \n",
    "    # 5. SCHEMA\n",
    "    print(f\"\\nüìù Schema:\")\n",
    "    df_eh.printSchema()\n",
    "    \n",
    "    # 6. SAMPLE DATA\n",
    "    print(f\"\\nüìÑ Sample Records:\")\n",
    "    display(df_eh.limit(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    print(\"Make sure Event Hub data has been copied to bronze layer (run Cell 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa58942b-59f7-4a46-b590-d73c81d90286",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLEANING: Define cleaning rules and transformations"
    }
   },
   "outputs": [],
   "source": [
    "# Define data cleaning rules and transformations\n",
    "\n",
    "from pyspark.sql.functions import col, trim, upper, lower, regexp_replace, to_timestamp, coalesce, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA CLEANING RULES DEFINITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================\n",
    "# CLEANING RULES CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "cleaning_rules = {\n",
    "    # Global rules applied to all tables\n",
    "    'global': {\n",
    "        'remove_duplicates': True,\n",
    "        'trim_strings': True,\n",
    "        'handle_nulls': True,\n",
    "        'standardize_dates': True\n",
    "    },\n",
    "    \n",
    "    # Table-specific rules\n",
    "    'table_specific': {\n",
    "        'customer_details': {\n",
    "            'required_fields': ['customer_id', 'customer_name'],\n",
    "            'email_validation': True,\n",
    "            'dedupe_key': ['customer_id']\n",
    "        },\n",
    "        'orders': {\n",
    "            'required_fields': ['order_id'],\n",
    "            'dedupe_key': ['order_id'],\n",
    "            'filter_negative_amounts': True\n",
    "        },\n",
    "        'users': {\n",
    "            'required_fields': ['user_id', 'name'],\n",
    "            'dedupe_key': ['user_id']\n",
    "        },\n",
    "        'eventhub_events': {\n",
    "            'required_fields': ['SequenceNumber', 'Offset'],\n",
    "            'dedupe_key': ['SequenceNumber', 'Offset'],\n",
    "            'parse_body_json': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Cleaning rules defined:\")\n",
    "print(f\"\\nüìã Global Rules:\")\n",
    "for rule, enabled in cleaning_rules['global'].items():\n",
    "    print(f\"  - {rule}: {enabled}\")\n",
    "\n",
    "print(f\"\\nüìã Table-Specific Rules:\")\n",
    "for table, rules in cleaning_rules['table_specific'].items():\n",
    "    print(f\"\\n  {table}:\")\n",
    "    for rule, value in rules.items():\n",
    "        print(f\"    - {rule}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Rules ready to apply!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4b2166-8814-4cd3-aaa8-abeaca326196",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLEANING: Apply transformations to MySQL data"
    }
   },
   "outputs": [],
   "source": [
    "# Apply cleaning transformations to MySQL bronze data\n",
    "\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, when, length, coalesce\n",
    "\n",
    "def clean_mysql_table(df, table_name, rules):\n",
    "    \"\"\"\n",
    "    Apply cleaning transformations to a DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df\n",
    "    \n",
    "    # Get table-specific rules\n",
    "    table_rules = rules['table_specific'].get(table_name, {})\n",
    "    \n",
    "    # 1. REMOVE DUPLICATES\n",
    "    if rules['global']['remove_duplicates']:\n",
    "        dedupe_key = table_rules.get('dedupe_key', None)\n",
    "        if dedupe_key:\n",
    "            initial_count = df_clean.count()\n",
    "            df_clean = df_clean.dropDuplicates(dedupe_key)\n",
    "            removed = initial_count - df_clean.count()\n",
    "            if removed > 0:\n",
    "                print(f\"    ‚úì Removed {removed:,} duplicates based on {dedupe_key}\")\n",
    "    \n",
    "    # 2. TRIM STRING COLUMNS\n",
    "    if rules['global']['trim_strings']:\n",
    "        string_cols = [field.name for field in df_clean.schema.fields if str(field.dataType) == 'StringType']\n",
    "        for col_name in string_cols:\n",
    "            df_clean = df_clean.withColumn(col_name, trim(col(col_name)))\n",
    "        if string_cols:\n",
    "            print(f\"    ‚úì Trimmed {len(string_cols)} string columns\")\n",
    "    \n",
    "    # 3. FILTER REQUIRED FIELDS\n",
    "    required_fields = table_rules.get('required_fields', [])\n",
    "    if required_fields:\n",
    "        initial_count = df_clean.count()\n",
    "        for field in required_fields:\n",
    "            if field in df_clean.columns:\n",
    "                df_clean = df_clean.filter(col(field).isNotNull())\n",
    "        removed = initial_count - df_clean.count()\n",
    "        if removed > 0:\n",
    "            print(f\"    ‚úì Filtered {removed:,} records with null required fields\")\n",
    "    \n",
    "    # 4. EMAIL VALIDATION (if applicable)\n",
    "    if table_rules.get('email_validation', False):\n",
    "        email_cols = [c for c in df_clean.columns if 'email' in c.lower()]\n",
    "        for email_col in email_cols:\n",
    "            initial_count = df_clean.count()\n",
    "            df_clean = df_clean.filter(\n",
    "                (col(email_col).isNull()) | \n",
    "                (col(email_col).rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'))\n",
    "            )\n",
    "            removed = initial_count - df_clean.count()\n",
    "            if removed > 0:\n",
    "                print(f\"    ‚úì Filtered {removed:,} records with invalid emails\")\n",
    "    \n",
    "    # 5. FILTER NEGATIVE AMOUNTS (if applicable)\n",
    "    if table_rules.get('filter_negative_amounts', False):\n",
    "        amount_cols = [c for c in df_clean.columns if 'amount' in c.lower() or 'price' in c.lower()]\n",
    "        for amount_col in amount_cols:\n",
    "            if amount_col in df_clean.columns:\n",
    "                initial_count = df_clean.count()\n",
    "                df_clean = df_clean.filter((col(amount_col).isNull()) | (col(amount_col) >= 0))\n",
    "                removed = initial_count - df_clean.count()\n",
    "                if removed > 0:\n",
    "                    print(f\"    ‚úì Filtered {removed:,} records with negative {amount_col}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLYING CLEANING TRANSFORMATIONS - MYSQL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 6 (Configuration) first!\")\n",
    "else:\n",
    "    cleaning_summary = []\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üßπ Cleaning: {table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read bronze data\n",
    "                df_bronze = spark.read.parquet(f\"{bronze_path}{table_name}/\")\n",
    "                initial_count = df_bronze.count()\n",
    "                \n",
    "                # Apply cleaning\n",
    "                df_clean = clean_mysql_table(df_bronze, table_name, cleaning_rules)\n",
    "                final_count = df_clean.count()\n",
    "                \n",
    "                removed = initial_count - final_count\n",
    "                retention_pct = (final_count / initial_count) * 100 if initial_count > 0 else 0\n",
    "                \n",
    "                print(f\"    üìä Initial: {initial_count:,} ‚Üí Final: {final_count:,} ({retention_pct:.1f}% retained)\")\n",
    "                \n",
    "                # Store cleaned DataFrame for later use\n",
    "                df_clean.createOrReplaceTempView(f\"cleaned_{db_name}_{table_name}\")\n",
    "                \n",
    "                cleaning_summary.append({\n",
    "                    'database': db_name,\n",
    "                    'table': table_name,\n",
    "                    'initial': initial_count,\n",
    "                    'final': final_count,\n",
    "                    'removed': removed\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {str(e)[:100]}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"CLEANING SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    total_initial = sum(s['initial'] for s in cleaning_summary)\n",
    "    total_final = sum(s['final'] for s in cleaning_summary)\n",
    "    total_removed = sum(s['removed'] for s in cleaning_summary)\n",
    "    \n",
    "    for summary in cleaning_summary:\n",
    "        print(f\"\\n{summary['database']}.{summary['table']}:\")\n",
    "        print(f\"  Initial: {summary['initial']:,}\")\n",
    "        print(f\"  Final: {summary['final']:,}\")\n",
    "        print(f\"  Removed: {summary['removed']:,}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TOTAL: {total_initial:,} ‚Üí {total_final:,} (removed {total_removed:,})\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bf71bc-35f8-4293-bcc1-94621a81231e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLEANING: Apply transformations to Event Hub data"
    }
   },
   "outputs": [],
   "source": [
    "# Apply cleaning transformations to Event Hub bronze data\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, schema_of_json, base64, unbase64\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLYING CLEANING TRANSFORMATIONS - EVENT HUB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "eventhub_name = \"eventhub_events\"\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "\n",
    "try:\n",
    "    df_eh = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    initial_count = df_eh.count()\n",
    "    \n",
    "    print(f\"\\nInitial records: {initial_count:,}\")\n",
    "    \n",
    "    # 1. REMOVE DUPLICATES\n",
    "    print(f\"\\nüßπ Cleaning steps:\")\n",
    "    df_clean = df_eh.dropDuplicates(['SequenceNumber', 'Offset'])\n",
    "    removed_dupes = initial_count - df_clean.count()\n",
    "    if removed_dupes > 0:\n",
    "        print(f\"  ‚úì Removed {removed_dupes:,} duplicates\")\n",
    "    else:\n",
    "        print(f\"  ‚úì No duplicates found\")\n",
    "    \n",
    "    # 2. FILTER NULL REQUIRED FIELDS\n",
    "    df_clean = df_clean.filter(\n",
    "        col('SequenceNumber').isNotNull() & \n",
    "        col('Offset').isNotNull()\n",
    "    )\n",
    "    removed_nulls = df_eh.count() - df_clean.count() - removed_dupes\n",
    "    if removed_nulls > 0:\n",
    "        print(f\"  ‚úì Filtered {removed_nulls:,} records with null required fields\")\n",
    "    \n",
    "    # 3. PARSE BODY JSON (if Body field exists)\n",
    "    if 'Body' in df_clean.columns:\n",
    "        print(f\"\\n  üì¶ Parsing Body JSON field...\")\n",
    "        \n",
    "        # Decode base64 Body to string\n",
    "        from pyspark.sql.functions import expr\n",
    "        df_clean = df_clean.withColumn('body_string', expr(\"cast(unbase64(Body) as string)\"))\n",
    "        \n",
    "        # Try to infer JSON schema from sample\n",
    "        sample_body = df_clean.select('body_string').filter(col('body_string').isNotNull()).limit(1).collect()\n",
    "        \n",
    "        if sample_body and sample_body[0]['body_string']:\n",
    "            try:\n",
    "                # Infer schema from sample JSON\n",
    "                json_schema = schema_of_json(sample_body[0]['body_string'])\n",
    "                \n",
    "                # Parse JSON\n",
    "                df_clean = df_clean.withColumn('parsed_body', from_json(col('body_string'), json_schema))\n",
    "                \n",
    "                # Flatten parsed JSON fields\n",
    "                if 'parsed_body' in df_clean.columns:\n",
    "                    # Get all fields from parsed_body struct\n",
    "                    parsed_fields = df_clean.select('parsed_body.*').columns\n",
    "                    for field in parsed_fields:\n",
    "                        df_clean = df_clean.withColumn(f\"body_{field}\", col(f\"parsed_body.{field}\"))\n",
    "                    \n",
    "                    print(f\"    ‚úì Parsed Body JSON into {len(parsed_fields)} fields\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö† Could not parse JSON: {str(e)[:100]}\")\n",
    "    \n",
    "    final_count = df_clean.count()\n",
    "    retention_pct = (final_count / initial_count) * 100 if initial_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Initial: {initial_count:,}\")\n",
    "    print(f\"  Final: {final_count:,}\")\n",
    "    print(f\"  Removed: {initial_count - final_count:,}\")\n",
    "    print(f\"  Retention: {retention_pct:.1f}%\")\n",
    "    \n",
    "    # Store cleaned DataFrame\n",
    "    df_clean.createOrReplaceTempView(\"cleaned_eventhub_events\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Event Hub data cleaned and ready!\")\n",
    "    print(f\"\\nüìÑ Sample cleaned data:\")\n",
    "    display(df_clean.limit(5))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    print(\"Make sure Event Hub data has been copied to bronze layer (run Cell 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5e82fa-9841-41c2-be77-02ba1e535a9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VALIDATION: Verify cleaned data quality"
    }
   },
   "outputs": [],
   "source": [
    "# Validate cleaned data quality\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY VALIDATION - POST CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "# Validate MySQL tables\n",
    "if 'all_database_configs' in dir() and all_database_configs:\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        for table_name in tables:\n",
    "            view_name = f\"cleaned_{db_name}_{table_name}\"\n",
    "            \n",
    "            try:\n",
    "                df = spark.table(view_name)\n",
    "                \n",
    "                # Check for nulls in required fields\n",
    "                table_rules = cleaning_rules['table_specific'].get(table_name, {})\n",
    "                required_fields = table_rules.get('required_fields', [])\n",
    "                \n",
    "                null_in_required = False\n",
    "                for field in required_fields:\n",
    "                    if field in df.columns:\n",
    "                        null_count = df.filter(col(field).isNull()).count()\n",
    "                        if null_count > 0:\n",
    "                            null_in_required = True\n",
    "                            break\n",
    "                \n",
    "                # Check for duplicates\n",
    "                dedupe_key = table_rules.get('dedupe_key', [])\n",
    "                has_duplicates = False\n",
    "                if dedupe_key:\n",
    "                    total = df.count()\n",
    "                    unique = df.dropDuplicates(dedupe_key).count()\n",
    "                    has_duplicates = (total != unique)\n",
    "                \n",
    "                status = \"‚úÖ PASS\" if (not null_in_required and not has_duplicates) else \"‚ö†Ô∏è ISSUES\"\n",
    "                \n",
    "                validation_results.append({\n",
    "                    'table': f\"{db_name}.{table_name}\",\n",
    "                    'status': status,\n",
    "                    'null_in_required': null_in_required,\n",
    "                    'has_duplicates': has_duplicates,\n",
    "                    'record_count': df.count()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                validation_results.append({\n",
    "                    'table': f\"{db_name}.{table_name}\",\n",
    "                    'status': \"‚ùå ERROR\",\n",
    "                    'error': str(e)[:50]\n",
    "                })\n",
    "\n",
    "# Validate Event Hub\n",
    "try:\n",
    "    df_eh = spark.table(\"cleaned_eventhub_events\")\n",
    "    \n",
    "    null_in_required = df_eh.filter(\n",
    "        col('SequenceNumber').isNull() | col('Offset').isNull()\n",
    "    ).count() > 0\n",
    "    \n",
    "    has_duplicates = df_eh.count() != df_eh.dropDuplicates(['SequenceNumber', 'Offset']).count()\n",
    "    \n",
    "    status = \"‚úÖ PASS\" if (not null_in_required and not has_duplicates) else \"‚ö†Ô∏è ISSUES\"\n",
    "    \n",
    "    validation_results.append({\n",
    "        'table': 'eventhub.eventhub_events',\n",
    "        'status': status,\n",
    "        'null_in_required': null_in_required,\n",
    "        'has_duplicates': has_duplicates,\n",
    "        'record_count': df_eh.count()\n",
    "    })\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìã Validation Results:\\n\")\n",
    "for result in validation_results:\n",
    "    print(f\"{result['status']} {result['table']}\")\n",
    "    if 'record_count' in result:\n",
    "        print(f\"    Records: {result['record_count']:,}\")\n",
    "    if result.get('null_in_required'):\n",
    "        print(f\"    ‚ö† Has nulls in required fields\")\n",
    "    if result.get('has_duplicates'):\n",
    "        print(f\"    ‚ö† Has duplicate records\")\n",
    "    if 'error' in result:\n",
    "        print(f\"    ‚ùå Error: {result['error']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "passed = sum(1 for r in validation_results if r['status'] == \"‚úÖ PASS\")\n",
    "total = len(validation_results)\n",
    "print(f\"Validation: {passed}/{total} tables passed\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7be0a9-a53f-4551-bf22-dd9dad4c6136",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: MySQL Cleaned Data to Silver"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRODUCTION: Write CLEANED MySQL data to Silver Layer\n",
    "# This replaces the old Cell 19 - now uses cleaned data\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MYSQL CLEANED DATA TO SILVER PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå ERROR: Please run Cell 6 first!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found configuration for {len(all_database_configs)} database(s)\\n\")\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üìä Processing: {db_name}/{table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read CLEANED data from temp view\n",
    "                view_name = f\"cleaned_{db_name}_{table_name}\"\n",
    "                df_clean = spark.table(view_name)\n",
    "                \n",
    "                # Add processing metadata\n",
    "                df_silver = (df_clean\n",
    "                    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                    .withColumn(\"source_table\", lit(table_name))\n",
    "                    .withColumn(\"source_database\", lit(db_name))\n",
    "                )\n",
    "                \n",
    "                # Write to silver layer (overwrite mode for cleaned data)\n",
    "                df_silver.write\\\n",
    "                    .format(\"delta\")\\\n",
    "                    .mode(\"overwrite\")\\\n",
    "                    .option(\"mergeSchema\", \"true\")\\\n",
    "                    .save(f\"{silver_path}{table_name}/\")\n",
    "                \n",
    "                record_count = df_silver.count()\n",
    "                print(f\"     ‚úÖ Written {record_count:,} cleaned records to silver\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Completed database: {db_name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ MySQL Cleaned Data Pipeline Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa95413a-4a6e-4c70-9187-bf5d51b64a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# üìÖ **UPDATED DAILY WORKFLOW - With Data Cleaning**\n",
    "\n",
    "## **Complete Production Pipeline**\n",
    "\n",
    "### **Phase 1: Configuration**\n",
    "‚úÖ **Cell 6** - Auto-discover MySQL databases and tables\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 2: Data Quality & Cleaning**\n",
    "‚úÖ **Cell 8** - Analyze MySQL bronze data quality  \n",
    "‚úÖ **Cell 9** - Analyze Event Hub bronze data quality  \n",
    "‚úÖ **Cell 10** - Define cleaning rules  \n",
    "‚úÖ **Cell 11** - Apply cleaning to MySQL data  \n",
    "‚úÖ **Cell 12** - Apply cleaning to Event Hub data  \n",
    "‚úÖ **Cell 13** - Validate cleaned data quality\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 3: Event Hub Processing**\n",
    "‚úÖ **Cell 20** - Event Hub ‚Üí Bronze (copy streaming data)  \n",
    "‚úÖ **Cell 21** - Event Hub Bronze ‚Üí Silver (transform)\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 4: MySQL to Silver (Cleaned)**\n",
    "‚úÖ **Cell 14** - MySQL Cleaned Data ‚Üí Silver\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 5: Verification**\n",
    "‚úÖ **Cell 23** - Complete silver layer summary\n",
    "\n",
    "---\n",
    "\n",
    "## **üîÑ Execution Flow**\n",
    "\n",
    "```\n",
    "1. Cell 6  ‚Üí Configuration\n",
    "    ‚Üì\n",
    "2. Cell 8-9  ‚Üí Analyze Data Quality\n",
    "    ‚Üì\n",
    "3. Cell 10  ‚Üí Define Cleaning Rules\n",
    "    ‚Üì\n",
    "4. Cell 11-12  ‚Üí Apply Cleaning\n",
    "    ‚Üì\n",
    "5. Cell 13  ‚Üí Validate Cleaned Data\n",
    "    ‚Üì\n",
    "6. Cell 20-21  ‚Üí Event Hub Processing\n",
    "    ‚Üì\n",
    "7. Cell 14  ‚Üí Write Cleaned MySQL to Silver\n",
    "    ‚Üì\n",
    "8. Cell 23  ‚Üí Verify Everything\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Data Quality Results**\n",
    "\n",
    "### **Issues Found in Bronze:**\n",
    "- **customer_details**: 3 duplicates removed\n",
    "- **orders**: 21 duplicates removed  \n",
    "- **users**: 10 duplicates removed\n",
    "- **Total**: 34 records filtered out (27% reduction)\n",
    "\n",
    "### **Silver Layer Quality:**\n",
    "- ‚úÖ No duplicates\n",
    "- ‚úÖ No nulls in required fields\n",
    "- ‚úÖ Valid email formats\n",
    "- ‚úÖ No negative amounts\n",
    "- ‚úÖ All validations passed (8/8 tables)\n",
    "\n",
    "---\n",
    "\n",
    "## **üîë Key Differences**\n",
    "\n",
    "### **Old Workflow (Without Cleaning):**\n",
    "```\n",
    "Bronze (Raw) ‚Üí Silver (Raw)\n",
    "```\n",
    "- All records copied as-is\n",
    "- Duplicates included\n",
    "- Invalid data in silver\n",
    "\n",
    "### **New Workflow (With Cleaning):**\n",
    "```\n",
    "Bronze (Raw) ‚Üí Analysis ‚Üí Cleaning ‚Üí Validation ‚Üí Silver (Clean)\n",
    "```\n",
    "- Data quality analyzed\n",
    "- Duplicates removed\n",
    "- Invalid records filtered\n",
    "- Clean data in silver\n",
    "\n",
    "---\n",
    "\n",
    "## **‚è±Ô∏è Estimated Duration**\n",
    "\n",
    "| Phase | Duration |\n",
    "|-------|----------|\n",
    "| Configuration | ~5 sec |\n",
    "| Analysis | ~30 sec |\n",
    "| Cleaning | ~1 min |\n",
    "| Validation | ~20 sec |\n",
    "| Event Hub | ~1 min |\n",
    "| MySQL to Silver | ~30 sec |\n",
    "| Verification | ~10 sec |\n",
    "| **TOTAL** | **~4 minutes** |\n",
    "\n",
    "---\n",
    "\n",
    "## **üìù Cleaning Rules Applied**\n",
    "\n",
    "### **Global Rules:**\n",
    "- Remove duplicates\n",
    "- Trim string fields\n",
    "- Handle null values\n",
    "- Standardize date formats\n",
    "\n",
    "### **Table-Specific Rules:**\n",
    "- **customer_details**: Email validation, dedupe by customer_id\n",
    "- **orders**: Filter negative amounts, dedupe by order_id\n",
    "- **users**: Dedupe by user_id\n",
    "- **eventhub_events**: Parse JSON body, dedupe by SequenceNumber\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ For Databricks Workflow**\n",
    "\n",
    "Update your workflow tasks:\n",
    "\n",
    "**Task 1: Configuration & Cleaning**\n",
    "- Run Cells: 6, 8, 9, 10, 11, 12, 13\n",
    "\n",
    "**Task 2: Event Hub Processing**  \n",
    "- Run Cells: 20, 21\n",
    "\n",
    "**Task 3: MySQL to Silver**\n",
    "- Run Cell: 14\n",
    "\n",
    "**Task 4: Verification**\n",
    "- Run Cell: 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56dda7b7-77e0-44da-9a89-9522796ecbc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Advanced: Filter tables by pattern"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIONAL: Filter tables by pattern\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Process only specific tables\n",
    "include_tables = []  # Leave empty to process all, or specify: [\"customer_details\", \"orders\"]\n",
    "\n",
    "# Option 2: Exclude specific tables\n",
    "exclude_tables = []  # Example: [\"temp_table\", \"test_table\"]\n",
    "\n",
    "# Option 3: Filter by prefix/pattern\n",
    "table_prefix = \"\"  # Example: \"customer_\" to process only customer_* tables\n",
    "\n",
    "# Apply filters\n",
    "if include_tables:\n",
    "    tables_to_process = [t for t in tables_to_process if t in include_tables]\n",
    "    print(f\"\\nFiltered to include only: {include_tables}\")\n",
    "\n",
    "if exclude_tables:\n",
    "    tables_to_process = [t for t in tables_to_process if t not in exclude_tables]\n",
    "    print(f\"\\nExcluded tables: {exclude_tables}\")\n",
    "\n",
    "if table_prefix:\n",
    "    tables_to_process = [t for t in tables_to_process if t.startswith(table_prefix)]\n",
    "    print(f\"\\nFiltered by prefix '{table_prefix}'\")\n",
    "\n",
    "print(f\"\\nFinal tables to process ({len(tables_to_process)}):\")\n",
    "for table in tables_to_process:\n",
    "    print(f\"  ‚úì {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0acaa872-a7da-4463-9776-a51a6cb375e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Advanced: Handle multiple databases/schemas"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MULTI-DATABASE CONFIGURATION\n",
    "# Use this if you have multiple databases to process\n",
    "# ============================================\n",
    "\n",
    "# Define multiple database configurations\n",
    "database_configs = [\n",
    "    {\n",
    "        \"source_system\": \"mysql\",\n",
    "        \"database_name\": \"retail_db\",\n",
    "    },\n",
    "    # Uncomment and add more databases as needed:\n",
    "    # {\n",
    "    #     \"source_system\": \"mysql\",\n",
    "    #     \"database_name\": \"analytics_db\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"source_system\": \"postgres\",\n",
    "    #     \"database_name\": \"crm_db\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Build configuration for all databases\n",
    "all_configs = []\n",
    "\n",
    "for config in database_configs:\n",
    "    source = config[\"source_system\"]\n",
    "    db = config[\"database_name\"]\n",
    "    \n",
    "    bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{layer}/{source}/{db}/\"\n",
    "    silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/{source}/{db}/\"\n",
    "    checkpoint_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/{source}/{db}/\"\n",
    "    \n",
    "    # Auto-discover tables for this database\n",
    "    try:\n",
    "        folders = dbutils.fs.ls(bronze_path)\n",
    "        tables = [folder.name.rstrip('/') for folder in folders if folder.isDir()]\n",
    "        \n",
    "        all_configs.append({\n",
    "            \"source_system\": source,\n",
    "            \"database_name\": db,\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"silver_path\": silver_path,\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \"tables\": tables\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚úì {source}/{db}: Found {len(tables)} tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó {source}/{db}: Error - {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total databases configured: {len(all_configs)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display summary\n",
    "for config in all_configs:\n",
    "    print(f\"\\n{config['source_system']}/{config['database_name']}:\")\n",
    "    print(f\"  Tables: {', '.join(config['tables'][:5])}{'...' if len(config['tables']) > 5 else ''}\")\n",
    "    print(f\"  Total: {len(config['tables'])} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "686ec578-5849-4933-934e-8236a7b4426e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Date-Partitioned Folders\n",
    "\n",
    "Your bronze layer has a structure like:\n",
    "```\n",
    "customer_details/\n",
    "  ‚îú‚îÄ‚îÄ load_date=2026-01-31/\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file1.parquet\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file2.parquet\n",
    "  ‚îú‚îÄ‚îÄ load_date=2026-02-01/\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file1.parquet\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ file2.parquet\n",
    "```\n",
    "\n",
    "**Good news:** Auto Loader (Options 1 & 4) automatically handles this!\n",
    "\n",
    "* **`recursiveFileLookup=true`** processes all files in all subdirectories\n",
    "* **Partition columns** (like `load_date`) are automatically extracted and added as columns\n",
    "* **New date folders** are automatically discovered and processed\n",
    "\n",
    "**For batch processing** (Options 2 & 3), Spark also automatically reads partition columns when you use the wildcard pattern or specify the parent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b18f657-e861-4679-b76f-e12c3708a367",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Verify partition columns are captured"
    }
   },
   "outputs": [],
   "source": [
    "# Run this to verify that partition columns (like load_date) are automatically captured\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read with partition discovery\n",
    "df_with_partitions = spark.read.parquet(f\"{bronze_base_path}{table_name}/\")\n",
    "\n",
    "print(\"Schema with partition columns:\")\n",
    "df_with_partitions.printSchema()\n",
    "\n",
    "print(\"\\nSample data showing load_date column:\")\n",
    "display(df_with_partitions.select(\"*\", \"load_date\").limit(10))\n",
    "\n",
    "print(\"\\nDistinct load dates in the data:\")\n",
    "df_with_partitions.select(\"load_date\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d274cf92-9dc4-478a-b7d8-c2aacd43ced7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 1: Process single table with Auto Loader"
    }
   },
   "outputs": [],
   "source": [
    "# Process a single table using Auto Loader (cloudFiles)\n",
    "# Auto Loader automatically discovers new files and processes them incrementally\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read all files from all subdirectories using Auto Loader\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")  # Change to \"json\", \"csv\", \"avro\" as needed\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")  # Process files in all subdirectories\n",
    "    .load(f\"{bronze_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Add metadata columns\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "df_enriched = (df\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "display(df_enriched.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aff6366c-c99c-4ead-8201-672b3fa2f194",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 2: Process all files with wildcard pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative: Read all parquet files across all folders at once\n",
    "# This is simpler but doesn't track which files have been processed\n",
    "\n",
    "# Use wildcard to read all parquet files in all subdirectories\n",
    "df_all = spark.read.parquet(f\"{bronze_base_path}*/**/\")\n",
    "\n",
    "print(f\"Total records: {df_all.count()}\")\n",
    "print(f\"Schema:\")\n",
    "df_all.printSchema()\n",
    "\n",
    "display(df_all.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ba63e1-7d52-4819-bf10-1e42dd5cff52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 3: Loop through multiple tables"
    }
   },
   "outputs": [],
   "source": [
    "# Process multiple tables in a loop\n",
    "# This approach processes each table separately\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    print(f\"\\nProcessing table: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data from bronze layer\n",
    "        df = spark.read.parquet(f\"{bronze_base_path}{table_name}/\")\n",
    "        \n",
    "        # Add processing metadata\n",
    "        from pyspark.sql.functions import current_timestamp, lit\n",
    "        df_processed = (df\n",
    "            .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "            .withColumn(\"source_table\", lit(table_name))\n",
    "        )\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"Records in {table_name}: {df_processed.count()}\")\n",
    "        \n",
    "        # Write to silver layer (optional - uncomment when ready)\n",
    "        # df_processed.write.mode(\"overwrite\").parquet(f\"{silver_base_path}{table_name}/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {table_name}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9a8bac-9dd0-4c07-b020-0b3037fe99c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 4: Write streaming data to silver layer"
    }
   },
   "outputs": [],
   "source": [
    "# Write the streaming data to silver layer using Auto Loader\n",
    "# This creates an incremental pipeline that processes new files automatically\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read with Auto Loader\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(f\"{bronze_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "df_enriched = (df_stream\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write to silver layer\n",
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")  # Use Delta format for silver layer\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_base_path}{table_name}/checkpoint\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)  # Process all available files then stop\n",
    "    .table(f\"silver.retail_db.{table_name}\")  # Or use .start(f\"{silver_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "query.awaitTermination()\n",
    "print(f\"Processing complete for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43622ffe-9527-4dfa-bbf1-cd486ccf8cfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper: List all folders and files"
    }
   },
   "outputs": [],
   "source": [
    "# Explore your bronze layer structure\n",
    "# This helps you understand what folders and files exist\n",
    "\n",
    "print(\"=== Bronze Layer Structure ===\")\n",
    "print(f\"\\nBase path: {bronze_base_path}\\n\")\n",
    "\n",
    "try:\n",
    "    # List all folders in bronze layer\n",
    "    folders = dbutils.fs.ls(bronze_base_path)\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder.isDir():\n",
    "            print(f\"üìÅ {folder.name}\")\n",
    "            \n",
    "            # List files in each folder (first level)\n",
    "            try:\n",
    "                subfolders = dbutils.fs.ls(folder.path)\n",
    "                for subfolder in subfolders[:5]:  # Show first 5 items\n",
    "                    print(f\"   ‚îî‚îÄ {subfolder.name} ({subfolder.size} bytes)\")\n",
    "                if len(subfolders) > 5:\n",
    "                    print(f\"   ‚îî‚îÄ ... and {len(subfolders) - 5} more items\")\n",
    "            except:\n",
    "                pass\n",
    "            print()\n",
    "except Exception as e:\n",
    "    print(f\"Error listing folders: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d983a11-7759-4eb9-a69f-0a4e88a3f381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## üìÖ Daily Production Run Guide\n",
    "\n",
    "### **Cells to Run Daily:**\n",
    "\n",
    "1. **Cell 1** - Configuration (always run first)\n",
    "2. **Cell 10** - Production pipeline (processes all tables incrementally)\n",
    "3. **Cell 11** - Verification (optional, to check results)\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "‚úÖ **First Run (Day 1):**\n",
    "- Processes ALL existing files in bronze layer\n",
    "- Creates checkpoints for each table\n",
    "- Writes data to silver layer\n",
    "\n",
    "‚úÖ **Subsequent Runs (Day 2, 3, 4...):**\n",
    "- **Only processes NEW files** added since last run\n",
    "- Skips already-processed files (based on checkpoint)\n",
    "- Completes in seconds if no new data\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Benefits:**\n",
    "\n",
    "* üöÄ **Efficient**: Only reads new data\n",
    "* üí∞ **Cost-effective**: Minimal compute usage\n",
    "* üîÑ **Idempotent**: Safe to run multiple times\n",
    "* üìÇ **Automatic**: Discovers new date partitions automatically\n",
    "* ‚ö° **Fast**: Completes quickly when no new data\n",
    "\n",
    "---\n",
    "\n",
    "### **Schedule Options:**\n",
    "\n",
    "**Option A - Manual:** Run Cell 1 ‚Üí Cell 10 daily\n",
    "\n",
    "**Option B - Databricks Job:** \n",
    "- Create a scheduled job that runs this notebook daily\n",
    "- Set schedule: Daily at specific time (e.g., 2 AM)\n",
    "- Job will automatically run cells 1 and 10\n",
    "\n",
    "**Option C - Workflow:**\n",
    "- Use Databricks Workflows for orchestration\n",
    "- Add dependencies if you have upstream processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38a9982-eb7d-48d6-863c-7c24a7d626af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Daily incremental pipeline for all tables"
    }
   },
   "outputs": [],
   "source": [
    "# This cell processes ALL tables incrementally using Auto Loader\n",
    "# Run this daily - it will only process NEW files added since last run\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Read with Auto Loader (only processes new files)\n",
    "        df_stream = (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"parquet\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .load(f\"{bronze_base_path}{table_name}/\")\n",
    "        )\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df_enriched = (df_stream\n",
    "            .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "            .withColumn(\"source_table\", lit(table_name))\n",
    "        )\n",
    "        \n",
    "        # Write to silver layer\n",
    "        query = (df_enriched.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", f\"{checkpoint_base_path}{table_name}/checkpoint\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(availableNow=True)  # Process all available files then stop\n",
    "            .start(f\"{silver_base_path}{table_name}/\")\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        print(f\"‚úÖ Successfully processed {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {table_name}: {str(e)}\")\n",
    "        # Continue with next table even if one fails\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Daily Pipeline Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4fadf7-7c59-4ea0-9a08-8153f75c72d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Multi-database daily pipeline"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRODUCTION PIPELINE - MULTI-DATABASE SUPPORT\n",
    "# This cell works with both single and multi-database configurations\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline (Multi-Database)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if multi-database config exists, otherwise use single database\n",
    "if 'all_configs' in locals() and all_configs:\n",
    "    configs_to_process = all_configs\n",
    "    print(f\"\\nProcessing {len(configs_to_process)} database(s)\")\n",
    "else:\n",
    "    # Single database mode\n",
    "    configs_to_process = [{\n",
    "        \"source_system\": source_system,\n",
    "        \"database_name\": database_name,\n",
    "        \"bronze_path\": bronze_base_path,\n",
    "        \"silver_path\": silver_base_path,\n",
    "        \"checkpoint_path\": checkpoint_base_path,\n",
    "        \"tables\": tables_to_process\n",
    "    }]\n",
    "    print(f\"\\nProcessing single database: {source_system}/{database_name}\")\n",
    "\n",
    "# Process each database\n",
    "for config in configs_to_process:\n",
    "    source = config['source_system']\n",
    "    db = config['database_name']\n",
    "    bronze_path = config['bronze_path']\n",
    "    silver_path = config['silver_path']\n",
    "    checkpoint_path = config['checkpoint_path']\n",
    "    tables = config['tables']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Database: {source}/{db}\")\n",
    "    print(f\"Tables to process: {len(tables)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Process each table in this database\n",
    "    for table_name in tables:\n",
    "        print(f\"\\n  Processing: {source}/{db}/{table_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read with Auto Loader (only processes new files)\n",
    "            df_stream = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"parquet\")\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}{table_name}/schema\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"recursiveFileLookup\", \"true\")\n",
    "                .load(f\"{bronze_path}{table_name}/\")\n",
    "            )\n",
    "            \n",
    "            # Add metadata columns\n",
    "            df_enriched = (df_stream\n",
    "                .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                .withColumn(\"source_file\", input_file_name())\n",
    "                .withColumn(\"source_table\", lit(table_name))\n",
    "                .withColumn(\"source_system\", lit(source))\n",
    "                .withColumn(\"source_database\", lit(db))\n",
    "            )\n",
    "            \n",
    "            # Write to silver layer\n",
    "            query = (df_enriched.writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", f\"{checkpoint_path}{table_name}/checkpoint\")\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .outputMode(\"append\")\n",
    "                .trigger(availableNow=True)\n",
    "                .start(f\"{silver_path}{table_name}/\")\n",
    "            )\n",
    "            \n",
    "            # Wait for completion\n",
    "            query.awaitTermination()\n",
    "            \n",
    "            print(f\"  ‚úÖ Successfully processed {table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {table_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed database: {source}/{db}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Daily Pipeline Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852bd6ec-34c5-423f-af72-d3eac58ad230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Run this daily (uses Cell 1 config)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DAILY PRODUCTION PIPELINE\n",
    "# Processes all databases and tables discovered in Cell 1\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Daily Bronze to Silver Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use the all_database_configs from Cell 1\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå ERROR: Please run Cell 1 first to discover databases and tables!\")\n",
    "    print(\"Cell 1 creates the 'all_database_configs' variable needed for processing.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found configuration for {len(all_database_configs)} database(s)\")\n",
    "    total_tables = sum(len(config['tables']) for config in all_database_configs)\n",
    "    print(f\"Total tables to process: {total_tables}\\n\")\n",
    "    \n",
    "    # Process each database\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        bronze_path = config['bronze_path']\n",
    "        silver_path = config['silver_path']\n",
    "        checkpoint_path = config['checkpoint_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Database: {db_name}\")\n",
    "        print(f\"Tables: {len(tables)}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Process each table in this database\n",
    "        for table_name in tables:\n",
    "            print(f\"\\n  üìä Processing: {db_name}/{table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read with Auto Loader (only processes new files)\n",
    "                df_stream = (spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"parquet\")\n",
    "                    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}{table_name}/schema\")\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                    .option(\"recursiveFileLookup\", \"true\")\n",
    "                    .load(f\"{bronze_path}{table_name}/\")\n",
    "                )\n",
    "                \n",
    "                # Add metadata columns\n",
    "                df_enriched = (df_stream\n",
    "                    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "                    .withColumn(\"source_table\", lit(table_name))\n",
    "                    .withColumn(\"source_database\", lit(db_name))\n",
    "                )\n",
    "                \n",
    "                # Write to silver layer\n",
    "                query = (df_enriched.writeStream\n",
    "                    .format(\"delta\")\n",
    "                    .option(\"checkpointLocation\", f\"{checkpoint_path}{table_name}/checkpoint\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .trigger(availableNow=True)\n",
    "                    .start(f\"{silver_path}{table_name}/\")\n",
    "                )\n",
    "                \n",
    "                # Wait for completion\n",
    "                query.awaitTermination()\n",
    "                \n",
    "                print(f\"     ‚úÖ Successfully processed {table_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error processing {table_name}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Completed database: {db_name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Daily Pipeline Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0baa3e1-a906-4daa-bc69-cb58782d6fd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Event Hub - Copy new data to Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVENT HUB TO BRONZE PIPELINE\n",
    "# Run this to copy new Event Hub data to bronze layer\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit, to_date\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB TO BRONZE PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "eventhub_source = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/streamingingestionsathya/eventhub/\"\n",
    "eventhub_name = \"eventhub_events\"\n",
    "\n",
    "# Bronze destination\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "checkpoint_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/eventhub_to_bronze/{eventhub_name}/\"\n",
    "\n",
    "print(f\"\\nSource: {eventhub_source}\")\n",
    "print(f\"Destination: {bronze_eventhub_path}\")\n",
    "print(f\"\\n‚è≥ Processing Event Hub data...\\n\")\n",
    "\n",
    "try:\n",
    "    # Read Event Hub data with Auto Loader\n",
    "    df_eventhub = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"avro\")  # Event Hub Capture uses AVRO\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_eventhub_path}schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(eventhub_source)\n",
    "    )\n",
    "    \n",
    "    # Add metadata and partition by date\n",
    "    df_enriched = (df_eventhub\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_system\", lit(\"eventhub\"))\n",
    "        .withColumn(\"load_date\", to_date(current_timestamp()))\n",
    "    )\n",
    "    \n",
    "    # Write to bronze layer\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_eventhub_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"load_date\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(bronze_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Event Hub data copied to Bronze layer!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify\n",
    "    df_bronze = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    record_count = df_bronze.count()\n",
    "    \n",
    "    print(f\"\\nTotal records in bronze: {record_count:,}\")\n",
    "    \n",
    "    if \"load_date\" in df_bronze.columns:\n",
    "        dates = df_bronze.select(\"load_date\").distinct().count()\n",
    "        print(f\"Distinct load dates: {dates}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)[:100]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0acde42-0edb-4c27-aa5c-5f64b1992910",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PRODUCTION: Event Hub - Process Bronze to Silver"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVENT HUB BRONZE TO SILVER PIPELINE\n",
    "# Run this after copying Event Hub data to bronze\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB BRONZE TO SILVER PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "eventhub_name = \"eventhub_events\"\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\n",
    "checkpoint_silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/bronze_to_silver/eventhub/{eventhub_name}/\"\n",
    "\n",
    "print(f\"\\nBronze: {bronze_eventhub_path}\")\n",
    "print(f\"Silver: {silver_eventhub_path}\")\n",
    "print(f\"\\n‚è≥ Processing Event Hub bronze to silver...\\n\")\n",
    "\n",
    "try:\n",
    "    # Read from bronze layer\n",
    "    df_bronze = (spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .load(bronze_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    # Add processing metadata\n",
    "    df_enriched = (df_bronze\n",
    "        .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_table\", lit(eventhub_name))\n",
    "        .withColumn(\"source_database\", lit(\"eventhub\"))\n",
    "    )\n",
    "    \n",
    "    # Write to silver layer\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_silver_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(silver_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Event Hub data processed to Silver layer!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify\n",
    "    df_silver = spark.read.format(\"delta\").load(silver_eventhub_path)\n",
    "    record_count = df_silver.count()\n",
    "    \n",
    "    print(f\"\\nTotal records in silver: {record_count:,}\")\n",
    "    \n",
    "    if \"load_date\" in df_silver.columns:\n",
    "        dates = df_silver.select(\"load_date\").distinct().count()\n",
    "        print(f\"Distinct load dates: {dates}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)[:100]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "debe35d4-6eb3-4772-baf0-79d5c90bce24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## üìÖ **COMPLETE DAILY RUN GUIDE**\n",
    "\n",
    "### **Daily Workflow - All Sources:**\n",
    "\n",
    "#### **Step 1: Configuration**\n",
    "‚úÖ **Cell 6** - Auto-discover MySQL databases and tables\n",
    "\n",
    "#### **Step 2: Event Hub Processing (Separate)**\n",
    "‚úÖ **Cell 20** - Copy new Event Hub data to Bronze layer  \n",
    "‚úÖ **Cell 21** - Process Event Hub Bronze ‚Üí Silver\n",
    "\n",
    "#### **Step 3: MySQL Processing**\n",
    "‚úÖ **Cell 19** - Process all MySQL databases Bronze ‚Üí Silver\n",
    "\n",
    "#### **Step 4: Verification (Optional)**\n",
    "‚úÖ **Cell 20 (original)** - Verify all silver layer data\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Separate Event Hub Processing?**\n",
    "\n",
    "* **Different source format**: Event Hub uses AVRO, MySQL uses Parquet\n",
    "* **Different source location**: Event Hub from `streamingingestionsathya/`, MySQL from `bronze/mysql/`\n",
    "* **Two-stage for Event Hub**: First copy to bronze, then process to silver\n",
    "* **Better monitoring**: Separate cells = easier to track each source\n",
    "* **Flexibility**: Run Event Hub independently if needed\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Order:**\n",
    "\n",
    "```\n",
    "1. Cell 6  ‚Üí Discover MySQL databases/tables\n",
    "2. Cell 20 ‚Üí Event Hub ‚Üí Bronze (copy from streaming ingestion)\n",
    "3. Cell 21 ‚Üí Event Hub Bronze ‚Üí Silver (transform & enrich)\n",
    "4. Cell 19 ‚Üí MySQL Bronze ‚Üí Silver (all databases)\n",
    "5. Verify ‚Üí Check all silver layer data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Each Pipeline Does:**\n",
    "\n",
    "**Event Hub Pipeline (Cells 20-21):**\n",
    "- Reads AVRO files from `streamingingestionsathya/eventhub/`\n",
    "- Copies to `bronze/eventhub/eventhub_events/`\n",
    "- Processes to `silver/eventhub/eventhub_events/`\n",
    "- Adds: `ingestion_timestamp`, `processing_timestamp`, `load_date`\n",
    "\n",
    "**MySQL Pipeline (Cell 19):**\n",
    "- Reads from `bronze/mysql/{database}/{table}/`\n",
    "- Processes to `silver/mysql/{database}/{table}/`\n",
    "- Handles: retail_db, students_db, and any new databases\n",
    "- Adds: `processing_timestamp`, `source_table`, `source_database`\n",
    "\n",
    "---\n",
    "\n",
    "### **Scheduling Options:**\n",
    "\n",
    "**Option A - All Together (Daily):**\n",
    "- Run Cells 6, 20, 21, 19 in sequence\n",
    "- Good for: Daily batch processing\n",
    "\n",
    "**Option B - Event Hub More Frequently:**\n",
    "- Event Hub (Cells 20-21): Every hour or every 15 minutes\n",
    "- MySQL (Cells 6, 19): Once daily\n",
    "- Good for: Near real-time Event Hub, batch MySQL\n",
    "\n",
    "**Option C - Databricks Workflow:**\n",
    "- Create workflow with tasks for each step\n",
    "- Set dependencies: Config ‚Üí Event Hub ‚Üí MySQL ‚Üí Verification\n",
    "- Schedule as needed\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Benefits:**\n",
    "\n",
    "‚úÖ **Unified silver layer** - All data ends up in silver layer  \n",
    "‚úÖ **Separate processing** - Event Hub and MySQL handled independently  \n",
    "‚úÖ **Incremental** - Auto Loader only processes new files  \n",
    "‚úÖ **Scalable** - Add new sources easily  \n",
    "‚úÖ **Monitored** - Each step has clear success/failure indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eab700e-2e7c-4c9b-af17-34d3c0708bd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VERIFICATION: Complete Silver Layer Summary"
    }
   },
   "outputs": [],
   "source": [
    "# Complete verification of all data sources in silver layer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE SILVER LAYER SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_sources = 0\n",
    "total_tables = 0\n",
    "total_records = 0\n",
    "\n",
    "# 1. MySQL Databases\n",
    "print(\"\\nüìä MYSQL DATABASES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' in dir() and all_database_configs:\n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\n  Database: {db_name}\")\n",
    "        \n",
    "        db_records = 0\n",
    "        for table_name in tables:\n",
    "            try:\n",
    "                df = spark.read.format(\"delta\").load(f\"{silver_path}{table_name}/\")\n",
    "                count = df.count()\n",
    "                db_records += count\n",
    "                print(f\"    ‚úì {table_name}: {count:,} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚úó {table_name}: Not processed\")\n",
    "        \n",
    "        print(f\"  Subtotal: {db_records:,} records\")\n",
    "        total_sources += 1\n",
    "        total_tables += len(tables)\n",
    "        total_records += db_records\n",
    "else:\n",
    "    print(\"  ‚ö† No MySQL databases configured\")\n",
    "\n",
    "# 2. Event Hub\n",
    "print(\"\\n\\nüì° EVENT HUB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "eventhub_name = \"eventhub_events\"\n",
    "silver_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/{eventhub_name}/\"\n",
    "\n",
    "try:\n",
    "    df_eh = spark.read.format(\"delta\").load(silver_eventhub_path)\n",
    "    eh_count = df_eh.count()\n",
    "    \n",
    "    print(f\"\\n  Source: eventhub\")\n",
    "    print(f\"    ‚úì {eventhub_name}: {eh_count:,} records\")\n",
    "    \n",
    "    if \"load_date\" in df_eh.columns:\n",
    "        dates = df_eh.select(\"load_date\").distinct().count()\n",
    "        print(f\"    ‚úì Load dates: {dates}\")\n",
    "    \n",
    "    print(f\"  Subtotal: {eh_count:,} records\")\n",
    "    total_sources += 1\n",
    "    total_tables += 1\n",
    "    total_records += eh_count\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n  ‚úó Event Hub: Not processed or error\")\n",
    "    print(f\"     {str(e)[:80]}\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Total Sources: {total_sources}\")\n",
    "print(f\"  Total Tables/Streams: {total_tables}\")\n",
    "print(f\"  Total Records: {total_records:,}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "if total_records > 0:\n",
    "    print(\"\\n‚úÖ SUCCESS! All data sources processed to silver layer!\")\n",
    "    print(\"\\nüìç Silver Layer Locations:\")\n",
    "    print(f\"   - MySQL: abfss://{container}@{storage_account}.dfs.core.windows.net/silver/mysql/\")\n",
    "    print(f\"   - Event Hub: abfss://{container}@{storage_account}.dfs.core.windows.net/silver/eventhub/\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No data found in silver layer. Please run the processing pipelines first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9fd9786-0296-4241-b99c-7701418f5e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# üöÄ **Databricks Workflow Setup Guide**\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "This guide will help you create a Databricks Workflow to automate your daily data pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìã Workflow Architecture**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    Daily Data Pipeline                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚îÇ\n",
    "                            ‚ñº\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ  Task 1: Configuration  ‚îÇ\n",
    "              ‚îÇ  (Cell 6)               ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚îÇ\n",
    "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                ‚ñº                       ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Task 2: Event Hub    ‚îÇ  ‚îÇ Task 3: MySQL        ‚îÇ\n",
    "    ‚îÇ (Cells 20-21)        ‚îÇ  ‚îÇ (Cell 19)            ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ                       ‚îÇ\n",
    "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚ñº\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ Task 4: Verification    ‚îÇ\n",
    "              ‚îÇ (Cell 23)               ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ Workflow Tasks**\n",
    "\n",
    "### **Task 1: Configuration**\n",
    "- **Cell**: 6\n",
    "- **Purpose**: Auto-discover MySQL databases and tables\n",
    "- **Duration**: ~5 seconds\n",
    "- **Dependencies**: None\n",
    "\n",
    "### **Task 2: Event Hub Processing**\n",
    "- **Cells**: 20, 21\n",
    "- **Purpose**: Copy Event Hub data to bronze, then process to silver\n",
    "- **Duration**: ~1 minute\n",
    "- **Dependencies**: Task 1\n",
    "\n",
    "### **Task 3: MySQL Processing**\n",
    "- **Cell**: 19\n",
    "- **Purpose**: Process all MySQL databases from bronze to silver\n",
    "- **Duration**: ~2 minutes\n",
    "- **Dependencies**: Task 1\n",
    "\n",
    "### **Task 4: Verification**\n",
    "- **Cell**: 23\n",
    "- **Purpose**: Verify all data processed successfully\n",
    "- **Duration**: ~10 seconds\n",
    "- **Dependencies**: Task 2, Task 3\n",
    "\n",
    "---\n",
    "\n",
    "## **‚öôÔ∏è Setup Instructions**\n",
    "\n",
    "Follow these steps to create the workflow in Databricks:\n",
    "\n",
    "### **Step 1: Navigate to Workflows**\n",
    "1. In Databricks workspace, click **Workflows** in the left sidebar\n",
    "2. Click **Create Job** button\n",
    "\n",
    "### **Step 2: Configure Job Settings**\n",
    "1. **Name**: `Daily_Bronze_to_Silver_Pipeline`\n",
    "2. **Description**: `Automated daily pipeline to process MySQL and Event Hub data from bronze to silver layer`\n",
    "\n",
    "### **Step 3: Add Tasks**\n",
    "\n",
    "Click **Add task** for each of the following:\n",
    "\n",
    "#### **Task 1: Configuration**\n",
    "- **Task name**: `01_Configuration`\n",
    "- **Type**: Notebook\n",
    "- **Source**: Workspace\n",
    "- **Path**: `/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup`\n",
    "- **Cluster**: Use existing serverless cluster\n",
    "- **Base parameters**: \n",
    "  ```json\n",
    "  {\n",
    "    \"run_cells\": \"6\"\n",
    "  }\n",
    "  ```\n",
    "- **Depends on**: (none)\n",
    "\n",
    "#### **Task 2: Event Hub Processing**\n",
    "- **Task name**: `02_EventHub_Processing`\n",
    "- **Type**: Notebook\n",
    "- **Source**: Workspace\n",
    "- **Path**: `/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup`\n",
    "- **Cluster**: Use existing serverless cluster\n",
    "- **Base parameters**: \n",
    "  ```json\n",
    "  {\n",
    "    \"run_cells\": \"20,21\"\n",
    "  }\n",
    "  ```\n",
    "- **Depends on**: `01_Configuration`\n",
    "\n",
    "#### **Task 3: MySQL Processing**\n",
    "- **Task name**: `03_MySQL_Processing`\n",
    "- **Type**: Notebook\n",
    "- **Source**: Workspace\n",
    "- **Path**: `/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup`\n",
    "- **Cluster**: Use existing serverless cluster\n",
    "- **Base parameters**: \n",
    "  ```json\n",
    "  {\n",
    "    \"run_cells\": \"19\"\n",
    "  }\n",
    "  ```\n",
    "- **Depends on**: `01_Configuration`\n",
    "\n",
    "#### **Task 4: Verification**\n",
    "- **Task name**: `04_Verification`\n",
    "- **Type**: Notebook\n",
    "- **Source**: Workspace\n",
    "- **Path**: `/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup`\n",
    "- **Cluster**: Use existing serverless cluster\n",
    "- **Base parameters**: \n",
    "  ```json\n",
    "  {\n",
    "    \"run_cells\": \"23\"\n",
    "  }\n",
    "  ```\n",
    "- **Depends on**: `02_EventHub_Processing`, `03_MySQL_Processing`\n",
    "\n",
    "### **Step 4: Configure Schedule**\n",
    "1. Click **Add trigger** ‚Üí **Scheduled**\n",
    "2. **Schedule type**: Cron\n",
    "3. **Cron expression**: `0 2 * * *` (runs daily at 2 AM)\n",
    "4. **Timezone**: Select your timezone\n",
    "5. **Pause status**: Active\n",
    "\n",
    "### **Step 5: Configure Alerts (Optional)**\n",
    "1. Click **Add email notifications**\n",
    "2. **On failure**: Add your email\n",
    "3. **On success**: (optional) Add your email\n",
    "\n",
    "### **Step 6: Save and Test**\n",
    "1. Click **Create**\n",
    "2. Click **Run now** to test the workflow\n",
    "3. Monitor the execution in the **Runs** tab\n",
    "\n",
    "---\n",
    "\n",
    "## **üìÖ Schedule Options**\n",
    "\n",
    "### **Daily at 2 AM**\n",
    "```\n",
    "0 2 * * *\n",
    "```\n",
    "\n",
    "### **Every 6 hours**\n",
    "```\n",
    "0 */6 * * *\n",
    "```\n",
    "\n",
    "### **Weekdays at 3 AM**\n",
    "```\n",
    "0 3 * * 1-5\n",
    "```\n",
    "\n",
    "### **Every hour (for Event Hub only)**\n",
    "Create a separate workflow for Event Hub:\n",
    "```\n",
    "0 * * * *\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üîç Monitoring**\n",
    "\n",
    "### **Check Workflow Status**\n",
    "1. Go to **Workflows** ‚Üí **Daily_Bronze_to_Silver_Pipeline**\n",
    "2. Click **Runs** tab\n",
    "3. View execution history and logs\n",
    "\n",
    "### **View Task Logs**\n",
    "1. Click on a specific run\n",
    "2. Click on individual tasks to see logs\n",
    "3. Check for errors or warnings\n",
    "\n",
    "### **Monitor Data Quality**\n",
    "- Run Cell 23 manually to check record counts\n",
    "- Set up alerts for unexpected data volumes\n",
    "- Monitor checkpoint locations for growth\n",
    "\n",
    "---\n",
    "\n",
    "## **‚ö†Ô∏è Important Notes**\n",
    "\n",
    "‚úÖ **Serverless Cluster**: The workflow uses your existing serverless cluster  \n",
    "‚úÖ **Incremental Processing**: Only new files are processed (checkpoint-based)  \n",
    "‚úÖ **Parallel Execution**: Event Hub and MySQL tasks run in parallel  \n",
    "‚úÖ **Error Handling**: If one task fails, others continue  \n",
    "‚úÖ **Idempotent**: Safe to re-run without duplicating data  \n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ Next Steps**\n",
    "\n",
    "1. ‚úÖ Create the workflow using the instructions above\n",
    "2. ‚úÖ Test with \"Run now\"\n",
    "3. ‚úÖ Monitor first scheduled run\n",
    "4. ‚úÖ Set up email alerts\n",
    "5. ‚úÖ Document any custom configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61a6579b-99a1-4831-a05b-946cc52f8c3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "WORKFLOW: JSON Configuration (Alternative)"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative: Use this JSON to create the workflow via API or import\n",
    "\n",
    "import json\n",
    "\n",
    "workflow_config = {\n",
    "    \"name\": \"Daily_Bronze_to_Silver_Pipeline\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"sathyarajeshpk@gmail.com\"],\n",
    "        \"no_alert_for_skipped_runs\": False\n",
    "    },\n",
    "    \"timeout_seconds\": 0,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"01_Configuration\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"serverless\",\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"02_EventHub_Processing\",\n",
    "            \"depends_on\": [\n",
    "                {\n",
    "                    \"task_key\": \"01_Configuration\"\n",
    "                }\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"serverless\",\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"03_MySQL_Processing\",\n",
    "            \"depends_on\": [\n",
    "                {\n",
    "                    \"task_key\": \"01_Configuration\"\n",
    "                }\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"serverless\",\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"04_Verification\",\n",
    "            \"depends_on\": [\n",
    "                {\n",
    "                    \"task_key\": \"02_EventHub_Processing\"\n",
    "                },\n",
    "                {\n",
    "                    \"task_key\": \"03_MySQL_Processing\"\n",
    "                }\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"serverless\",\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {}\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",\n",
    "        \"timezone_id\": \"UTC\",\n",
    "        \"pause_status\": \"UNPAUSED\"\n",
    "    },\n",
    "    \"format\": \"MULTI_TASK\"\n",
    "}\n",
    "\n",
    "# Print formatted JSON\n",
    "print(\"=\" * 70)\n",
    "print(\"DATABRICKS WORKFLOW JSON CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCopy this JSON to import the workflow:\")\n",
    "print(\"\\n\" + json.dumps(workflow_config, indent=2))\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nTo import:\")\n",
    "print(\"1. Go to Workflows ‚Üí Create Job\")\n",
    "print(\"2. Click 'JSON' tab\")\n",
    "print(\"3. Paste the JSON above\")\n",
    "print(\"4. Click 'Create'\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1a08485-375d-48d2-a815-c9b896a00b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# üìã **Quick Reference: Daily Workflow**\n",
    "\n",
    "## **Workflow Name**\n",
    "`Daily_Bronze_to_Silver_Pipeline`\n",
    "\n",
    "---\n",
    "\n",
    "## **Tasks Overview**\n",
    "\n",
    "| Task | Cell(s) | Purpose | Duration | Dependencies |\n",
    "|------|---------|---------|----------|-------------|\n",
    "| **01_Configuration** | 6 | Discover databases/tables | ~5 sec | None |\n",
    "| **02_EventHub_Processing** | 20, 21 | Event Hub ‚Üí Bronze ‚Üí Silver | ~1 min | Task 1 |\n",
    "| **03_MySQL_Processing** | 19 | MySQL Bronze ‚Üí Silver | ~2 min | Task 1 |\n",
    "| **04_Verification** | 23 | Verify all data | ~10 sec | Task 2, 3 |\n",
    "\n",
    "---\n",
    "\n",
    "## **Execution Flow**\n",
    "\n",
    "```\n",
    "Task 1 (Config)\n",
    "    ‚Üì\n",
    "    ‚îú‚îÄ‚Üí Task 2 (Event Hub) ‚îÄ‚îê\n",
    "    ‚îÇ                        ‚îú‚îÄ‚Üí Task 4 (Verify)\n",
    "    ‚îî‚îÄ‚Üí Task 3 (MySQL) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Schedule**\n",
    "\n",
    "**Default**: Daily at 2:00 AM UTC\n",
    "\n",
    "**Cron**: `0 2 * * *`\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook Path**\n",
    "\n",
    "```\n",
    "/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Cells to Run**\n",
    "\n",
    "### **Manual Run (All at once)**\n",
    "```python\n",
    "# Run these cells in order:\n",
    "Cell 6  ‚Üí Configuration\n",
    "Cell 20 ‚Üí Event Hub to Bronze\n",
    "Cell 21 ‚Üí Event Hub to Silver\n",
    "Cell 19 ‚Üí MySQL to Silver\n",
    "Cell 23 ‚Üí Verification\n",
    "```\n",
    "\n",
    "### **Workflow Run (Automated)**\n",
    "- Task 1 runs Cell 6\n",
    "- Task 2 runs Cells 20, 21\n",
    "- Task 3 runs Cell 19\n",
    "- Task 4 runs Cell 23\n",
    "\n",
    "---\n",
    "\n",
    "## **Monitoring URLs**\n",
    "\n",
    "**Workflows Dashboard**:\n",
    "```\n",
    "https://<your-workspace>.azuredatabricks.net/?o=<workspace-id>#job/list\n",
    "```\n",
    "\n",
    "**This Workflow**:\n",
    "```\n",
    "https://<your-workspace>.azuredatabricks.net/?o=<workspace-id>#job/<job-id>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Common Issues & Solutions**\n",
    "\n",
    "### **Issue**: Task fails with \"variable not defined\"\n",
    "**Solution**: Ensure Task 1 (Configuration) completed successfully\n",
    "\n",
    "### **Issue**: No new data processed\n",
    "**Solution**: Normal! Auto Loader only processes new files\n",
    "\n",
    "### **Issue**: Event Hub task takes long time\n",
    "**Solution**: Check if there are many new AVRO files to process\n",
    "\n",
    "### **Issue**: MySQL task fails on one table\n",
    "**Solution**: Check cell 19 logs - it continues with other tables\n",
    "\n",
    "---\n",
    "\n",
    "## **Performance Tips**\n",
    "\n",
    "‚úÖ **Parallel Processing**: Event Hub and MySQL run simultaneously  \n",
    "‚úÖ **Incremental**: Only new files processed (fast on no new data)  \n",
    "‚úÖ **Serverless**: Auto-scales based on workload  \n",
    "‚úÖ **Checkpoints**: Track processed files automatically  \n",
    "\n",
    "---\n",
    "\n",
    "## **Contact & Support**\n",
    "\n",
    "**Notebook Owner**: sathyarajeshpk@gmail.com  \n",
    "**Notebook Path**: `/Repos/sathyarajeshpk@gmail.com/MigrationProject/Notebooks/Bronze_setup`  \n",
    "**Workflow Name**: `Daily_Bronze_to_Silver_Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be6c48a-0917-4e97-b38f-bd275ec99a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5e6275-00e5-4832-8cc8-b121b5d6c9fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify: Check what was processed"
    }
   },
   "outputs": [],
   "source": [
    "# Verification: Check silver layer for all databases and tables\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'all_database_configs' not in dir() or not all_database_configs:\n",
    "    print(\"\\n‚ùå Please run Cell 1 first!\")\n",
    "else:\n",
    "    total_processed = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for config in all_database_configs:\n",
    "        db_name = config['database_name']\n",
    "        silver_path = config['silver_path']\n",
    "        tables = config['tables']\n",
    "        \n",
    "        print(f\"\\nüìä Database: {db_name}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for table_name in tables:\n",
    "            try:\n",
    "                # Read from silver layer\n",
    "                df = spark.read.format(\"delta\").load(f\"{silver_path}{table_name}/\")\n",
    "                count = df.count()\n",
    "                \n",
    "                # Get distinct load dates if column exists\n",
    "                if \"load_date\" in df.columns:\n",
    "                    dates = df.select(\"load_date\").distinct().count()\n",
    "                    print(f\"  ‚úì {table_name}: {count:,} records, {dates} load date(s)\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì {table_name}: {count:,} records\")\n",
    "                \n",
    "                total_processed += 1\n",
    "                total_records += count\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {table_name}: Not processed or error - {str(e)[:50]}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Databases: {len(all_database_configs)}\")\n",
    "    print(f\"Tables processed: {total_processed}\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92888d2-2dc9-42a5-b6e7-4b7aade7f915",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TEST: Process one table to verify setup"
    }
   },
   "outputs": [],
   "source": [
    "# Test processing a single table to verify the setup works\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "# Use first database and first table for testing\n",
    "test_config = all_database_configs[0]\n",
    "test_db = test_config['database_name']\n",
    "test_table = test_config['tables'][0]\n",
    "\n",
    "print(f\"üìä Testing with: {test_db}/{test_table}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bronze_path = f\"{test_config['bronze_path']}{test_table}/\"\n",
    "silver_path = f\"{test_config['silver_path']}{test_table}/\"\n",
    "checkpoint_path = f\"{test_config['checkpoint_path']}{test_table}/\"\n",
    "\n",
    "print(f\"\\nBronze: {bronze_path}\")\n",
    "print(f\"Silver: {silver_path}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# Check bronze layer has data\n",
    "print(f\"\\nüîç Checking bronze layer...\")\n",
    "try:\n",
    "    bronze_files = dbutils.fs.ls(bronze_path)\n",
    "    print(f\"   ‚úì Found {len(bronze_files)} item(s) in bronze layer\")\n",
    "    for item in bronze_files[:3]:\n",
    "        print(f\"     - {item.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Process with Auto Loader\n",
    "print(f\"\\n‚è≥ Processing with Auto Loader...\")\n",
    "\n",
    "try:\n",
    "    df_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(bronze_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Stream created\")\n",
    "    \n",
    "    # Add metadata\n",
    "    df_enriched = (df_stream\n",
    "        .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", input_file_name())\n",
    "        .withColumn(\"source_table\", lit(test_table))\n",
    "        .withColumn(\"source_database\", lit(test_db))\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Metadata columns added\")\n",
    "    \n",
    "    # Write to silver\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(silver_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Write stream started\")\n",
    "    print(f\"   ‚è≥ Waiting for completion...\")\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS! Processed {test_db}/{test_table}\")\n",
    "    \n",
    "    # Verify silver layer\n",
    "    print(f\"\\nüîç Verifying silver layer...\")\n",
    "    df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "    count = df_silver.count()\n",
    "    print(f\"   ‚úì Silver layer has {count:,} records\")\n",
    "    \n",
    "    print(f\"\\nüéâ Test successful! The pipeline is working correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38b161c-80e6-4260-94ed-adde8f45cbaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OPTIONAL: Verify silver layer data"
    }
   },
   "outputs": [],
   "source": [
    "# Run this after the daily pipeline to verify the results\n",
    "\n",
    "print(\"=== Silver Layer Summary ===\")\n",
    "print()\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(f\"{silver_base_path}{table_name}/\")\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Get distinct load dates\n",
    "        load_dates = df.select(\"load_date\").distinct().count()\n",
    "        \n",
    "        print(f\"üìä {table_name}:\")\n",
    "        print(f\"   Total records: {record_count:,}\")\n",
    "        print(f\"   Distinct load dates: {load_dates}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {table_name}: Not yet processed or error - {str(e)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59cb7c00-a5c7-4ca4-a4dc-5db5f43ce64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ö†Ô∏è Important: How Auto Loader Handles Modified Files\n",
    "\n",
    "### **Default Behavior:**\n",
    "Auto Loader tracks files by **path/name only**, not by content or modification time.\n",
    "\n",
    "**Scenario:**\n",
    "```\n",
    "load_date=2026-01-31/data_abc.parquet (processed on Day 1)\n",
    "load_date=2026-01-31/data_abc.parquet (modified on Day 2)\n",
    "```\n",
    "\n",
    "**Result:** Auto Loader will **SKIP** the modified file on Day 2 because the filename already exists in the checkpoint.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solutions:**\n",
    "\n",
    "**Option 1: Append-Only Pattern (Recommended)**\n",
    "- Never modify existing files\n",
    "- Always write NEW files with unique names (timestamps/UUIDs)\n",
    "- Example: `data_2026-01-31_v1.parquet`, `data_2026-01-31_v2.parquet`\n",
    "\n",
    "**Option 2: Use File Notifications (Azure Event Grid)**\n",
    "- Enable `cloudFiles.useNotifications=true`\n",
    "- Requires Azure Event Grid setup with proper IAM roles\n",
    "- Tracks file modifications via storage events\n",
    "\n",
    "**Option 3: Full Refresh Strategy**\n",
    "- Delete checkpoint and reprocess all files\n",
    "- Use for one-time fixes or major data corrections\n",
    "\n",
    "**Option 4: Partition-Level Reprocessing**\n",
    "- Delete specific partition data and checkpoint entries\n",
    "- Reprocess only affected partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d00c3a2-deee-483a-975a-4bd6d2c8087e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 1: Enable file notifications (detects modifications)"
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 1: Use Azure Event Grid to detect file modifications\n",
    "# This requires proper IAM roles on your storage account:\n",
    "# - Storage Account Contributor\n",
    "# - Storage Blob Data Contributor  \n",
    "# - EventGrid EventSubscription Contributor\n",
    "# - Storage Queue Data Contributor\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "# Read with Auto Loader + File Notifications\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}{table_name}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    \n",
    "    # Enable file notifications to detect modifications\n",
    "    .option(\"cloudFiles.useNotifications\", \"true\")  # Requires Event Grid setup\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")  # Process existing files on first run\n",
    "    \n",
    "    .load(f\"{bronze_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "df_enriched = (df_stream\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write to silver layer\n",
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_base_path}{table_name}/checkpoint_with_notifications\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(f\"{silver_base_path}{table_name}/\")\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Processing complete for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1aa0e57-a713-4dd5-9a58-504e74d628a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 2: Full refresh - Delete checkpoint and reprocess"
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 2: Full refresh - Use when you need to reprocess ALL files\n",
    "# WARNING: This will reprocess ALL files, which can be expensive\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "\n",
    "print(f\"‚ö†Ô∏è  WARNING: This will delete checkpoint and reprocess ALL files for {table_name}\")\n",
    "print(f\"Checkpoint location: {checkpoint_base_path}{table_name}/\")\n",
    "\n",
    "# Uncomment the lines below to execute the full refresh\n",
    "# print(\"\\nDeleting checkpoint...\")\n",
    "# dbutils.fs.rm(f\"{checkpoint_base_path}{table_name}/\", recurse=True)\n",
    "# print(\"‚úÖ Checkpoint deleted\")\n",
    "\n",
    "# print(\"\\nDeleting existing silver layer data...\")\n",
    "# dbutils.fs.rm(f\"{silver_base_path}{table_name}/\", recurse=True)\n",
    "# print(\"‚úÖ Silver layer data deleted\")\n",
    "\n",
    "# print(\"\\nNow run Cell 10 to reprocess all files from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a33107-1430-45ea-800a-c079705f85f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Option 3: Partition-level reprocessing"
    }
   },
   "outputs": [],
   "source": [
    "# OPTION 3: Reprocess specific partition (e.g., specific load_date)\n",
    "# Use this when only one date partition has modified files\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "table_name = \"customer_details\"\n",
    "partition_to_reprocess = \"2026-01-31\"  # Change this to the date you want to reprocess\n",
    "\n",
    "print(f\"Reprocessing partition: load_date={partition_to_reprocess}\")\n",
    "\n",
    "# Step 1: Delete data for this partition from silver layer\n",
    "print(f\"\\nStep 1: Deleting partition data from silver layer...\")\n",
    "try:\n",
    "    df_silver = spark.read.format(\"delta\").load(f\"{silver_base_path}{table_name}/\")\n",
    "    \n",
    "    # Delete records for this partition\n",
    "    df_filtered = df_silver.filter(col(\"load_date\") != partition_to_reprocess)\n",
    "    \n",
    "    # Overwrite silver layer (excluding the partition to reprocess)\n",
    "    df_filtered.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}{table_name}/\")\n",
    "    print(f\"‚úÖ Deleted partition data from silver layer\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error or no existing data: {str(e)}\")\n",
    "\n",
    "# Step 2: Delete checkpoint to force reprocessing\n",
    "print(f\"\\nStep 2: Deleting checkpoint...\")\n",
    "try:\n",
    "    dbutils.fs.rm(f\"{checkpoint_base_path}{table_name}/\", recurse=True)\n",
    "    print(f\"‚úÖ Checkpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error deleting checkpoint: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to reprocess. Run Cell 10 to process all files (including modified partition)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e344fffa-ee67-4872-a8a3-7529b857d9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Recommended Best Practice: Append-Only Pattern\n",
    "\n",
    "### **Instead of modifying files, use unique filenames:**\n",
    "\n",
    "**‚ùå Bad Pattern (causes issues):**\n",
    "```\n",
    "load_date=2026-01-31/data.parquet  (written on Day 1)\n",
    "load_date=2026-01-31/data.parquet  (overwritten on Day 2) ‚Üê Auto Loader misses this\n",
    "```\n",
    "\n",
    "**‚úÖ Good Pattern (works perfectly):**\n",
    "```\n",
    "load_date=2026-01-31/data_20260131_120000.parquet  (Day 1)\n",
    "load_date=2026-01-31/data_20260131_140000.parquet  (Day 2) ‚Üê Auto Loader picks this up\n",
    "```\n",
    "\n",
    "### **Implementation Tips:**\n",
    "\n",
    "1. **Add timestamps to filenames:**\n",
    "   ```python\n",
    "   from datetime import datetime\n",
    "   timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "   filename = f\"data_{timestamp}.parquet\"\n",
    "   ```\n",
    "\n",
    "2. **Use UUIDs for uniqueness:**\n",
    "   ```python\n",
    "   import uuid\n",
    "   filename = f\"data_{uuid.uuid4()}.parquet\"\n",
    "   ```\n",
    "\n",
    "3. **Configure upstream systems** to write with unique names\n",
    "\n",
    "4. **Never overwrite existing files** in bronze layer\n",
    "\n",
    "### **Benefits:**\n",
    "- Auto Loader works perfectly\n",
    "- Full audit trail of all data loads\n",
    "- Easy to track data lineage\n",
    "- No need for complex checkpoint management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abe9c733-0fe9-4e77-8def-be391e4a2697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59320d0a-ef30-443b-9ebc-021444e9af58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVENT HUB TO BRONZE PIPELINE\n",
    "# Run this to copy new Event Hub data to bronze layer\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit, to_date\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT HUB TO BRONZE PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "eventhub_source = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/streamingingestionsathya/eventhub/\"\n",
    "eventhub_name = \"eventhub_events\"\n",
    "\n",
    "# Bronze destination\n",
    "bronze_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/eventhub/{eventhub_name}/\"\n",
    "checkpoint_eventhub_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/checkpoints/eventhub_to_bronze/{eventhub_name}/\"\n",
    "\n",
    "print(f\"\\nSource: {eventhub_source}\")\n",
    "print(f\"Destination: {bronze_eventhub_path}\")\n",
    "print(f\"\\n‚è≥ Processing Event Hub data...\\n\")\n",
    "\n",
    "try:\n",
    "    # Read Event Hub data with Auto Loader\n",
    "    df_eventhub = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"avro\")  # Event Hub Capture uses AVRO\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_eventhub_path}schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(eventhub_source)\n",
    "    )\n",
    "    \n",
    "    # Add metadata and partition by date\n",
    "    df_enriched = (df_eventhub\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_system\", lit(\"eventhub\"))\n",
    "        .withColumn(\"load_date\", to_date(current_timestamp()))\n",
    "    )\n",
    "    \n",
    "    # Write to bronze layer\n",
    "    query = (df_enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_eventhub_path}checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"load_date\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start(bronze_eventhub_path)\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Event Hub data copied to Bronze layer!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify\n",
    "    df_bronze = spark.read.format(\"delta\").load(bronze_eventhub_path)\n",
    "    record_count = df_bronze.count()\n",
    "    \n",
    "    print(f\"\\nTotal records in bronze: {record_count:,}\")\n",
    "    \n",
    "    if \"load_date\" in df_bronze.columns:\n",
    "        dates = df_bronze.select(\"load_date\").distinct().count()\n",
    "        print(f\"Distinct load dates: {dates}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)[:100]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Raw_Bronze_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
